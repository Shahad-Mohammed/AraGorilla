"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "YituTech/conv-bert-base", "api_call": "AutoModel.from_pretrained('YituTech/conv-bert-base')", "api_arguments": "N/A", "python_environment_requirements": "transformers", "example_code": "N/A", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "dmis-lab/biobert-v1.1", "api_call": "AutoModel.from_pretrained('dmis-lab/biobert-v1.1')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "BioBERT is a pre-trained biomedical language representation model for biomedical text mining tasks such as biomedical named entity recognition, relation extraction, and question answering."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Natural Language Processing Sentence Similarity", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "princeton-nlp/unsup-simcse-roberta-base", "api_call": "AutoModel.from_pretrained('princeton-nlp/unsup-simcse-roberta-base')", "api_arguments": null, "python_environment_requirements": ["transformers"], "example_code": null, "performance": {"dataset": null, "accuracy": null}, "description": "An unsupervised sentence embedding model trained using the SimCSE approach with a Roberta base architecture."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "api_call": "AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')", "api_arguments": "input_ids, attention_mask", "python_environment_requirements": "transformers", "example_code": "inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]", "performance": {"dataset": "UMLS", "accuracy": "N/A"}, "description": "SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/bart-base", "api_call": "BartModel.from_pretrained('facebook/bart-base')", "api_arguments": ["inputs"], "python_environment_requirements": ["transformers"], "example_code": "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\nmodel = BartModel.from_pretrained('facebook/bart-base')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "arxiv", "accuracy": "Not provided"}, "description": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/bart-large", "api_call": "BartModel.from_pretrained('facebook/bart-large')", "api_arguments": {"pretrained_model_name": "facebook/bart-large"}, "python_environment_requirements": {"library": "transformers", "version": "latest"}, "example_code": "from transformers import BartTokenizer, BartModel\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel = BartModel.from_pretrained('facebook/bart-large')\ninputs = tokenizer(Hello, my dog is cute, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "arxiv", "accuracy": "Not provided"}, "description": "BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vits8", "api_call": "ViTModel.from_pretrained('facebook/dino-vits8')", "api_arguments": ["images", "return_tensors"], "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "imagenet-1k", "accuracy": null}, "description": "Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vitb16", "api_call": "ViTModel.from_pretrained('facebook/dino-vitb16')", "api_arguments": {"pretrained_model_name_or_path": "facebook/dino-vitb16"}, "python_environment_requirements": {"transformers": "latest", "PIL": "latest", "requests": "latest"}, "example_code": "from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "PyTorch Transformers", "functionality": "Feature Extraction", "api_name": "kobart-base-v2", "api_call": "BartModel.from_pretrained('gogamza/kobart-base-v2')", "api_arguments": {"tokenizer": "PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"}, "python_environment_requirements": {"transformers": "latest", "tokenizers": "latest"}, "example_code": "from transformers import PreTrainedTokenizerFast, BartModel\ntokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\nmodel = BartModel.from_pretrained('gogamza/kobart-base-v2')", "performance": {"dataset": "NSMC", "accuracy": 0.901}, "description": "KoBART is a Korean encoder-decoder language model trained on over 40GB of Korean text using the BART architecture. It can be used for feature extraction and has been trained on a variety of data sources, including Korean Wiki, news, books, and more."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Contextual Representation", "api_name": "indobenchmark/indobert-base-p1", "api_call": "AutoModel.from_pretrained('indobenchmark/indobert-base-p1')", "api_arguments": ["BertTokenizer", "AutoModel", "tokenizer.encode", "torch.LongTensor", "model(x)[0].sum()"], "python_environment_requirements": ["transformers", "torch"], "example_code": "from transformers import BertTokenizer, AutoModel\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\nmodel = AutoModel.from_pretrained('indobenchmark/indobert-base-p1')\nx = torch.LongTensor(tokenizer.encode('aku adalah anak [MASK]')).view(1,-1)\nprint(x, model(x)[0].sum())", "performance": {"dataset": "Indo4B", "accuracy": "23.43 GB of text"}, "description": "IndoBERT is a state-of-the-art language model for Indonesian based on the BERT model. The pretrained model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "microsoft/codebert-base", "api_call": "AutoModel.from_pretrained('microsoft/codebert-base')", "api_arguments": "n/a", "python_environment_requirements": ["transformers"], "example_code": "n/a", "performance": {"dataset": "CodeSearchNet", "accuracy": "n/a"}, "description": "Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages. The model is trained on bi-modal data (documents & code) of CodeSearchNet. This model is initialized with Roberta-base and trained with MLM+RTD objective."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "GanjinZero/UMLSBert_ENG", "api_call": "AutoModel.from_pretrained('GanjinZero/UMLSBert_ENG')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "CODER: Knowledge infused cross-lingual medical term embedding for term normalization. English Version. Old name. This model is not UMLSBert! Github Link: https://github.com/GanjinZero/CODER"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "hubert-large-ll60k", "api_call": "HubertModel.from_pretrained('facebook/hubert-large-ll60k')", "api_arguments": "pretrained model name", "python_environment_requirements": "transformers", "example_code": "hubert = HubertModel.from_pretrained('facebook/hubert-large-ll60k')", "performance": {"dataset": "Libri-Light", "accuracy": "matches or improves upon the state-of-the-art wav2vec 2.0 performance"}, "description": "Hubert-Large is a self-supervised speech representation learning model pretrained on 16kHz sampled speech audio. It is designed to deal with the unique problems in speech representation learning, such as multiple sound units in each input utterance, no lexicon of input sound units during the pre-training phase, and variable lengths of sound units with no explicit segmentation. The model relies on an offline clustering step to provide aligned target labels for a BERT-like prediction loss."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "sup-simcse-roberta-large", "api_call": "AutoModel.from_pretrained('princeton-nlp/sup-simcse-roberta-large')", "api_arguments": ["AutoTokenizer", "AutoModel"], "python_environment_requirements": ["transformers"], "example_code": "from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(princeton-nlp/sup-simcse-roberta-large)\nmodel = AutoModel.from_pretrained(princeton-nlp/sup-simcse-roberta-large)", "performance": {"dataset": "STS tasks", "accuracy": "Spearman's correlation (See associated paper Appendix B)"}, "description": "A pretrained RoBERTa-large model for simple contrastive learning of sentence embeddings. It can be used for feature extraction and has been evaluated on semantic textual similarity (STS) tasks and downstream transfer tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "DeepPavlov/rubert-base-cased", "api_call": "AutoModel.from_pretrained('DeepPavlov/rubert-base-cased')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "Russian part of Wikipedia and news data", "accuracy": ""}, "description": "RuBERT (Russian, cased, 12â€‘layer, 768â€‘hidden, 12â€‘heads, 180M parameters) was trained on the Russian part of Wikipedia and news data. We used this training data to build a vocabulary of Russian subtokens and took a multilingual version of BERTâ€‘base as an initialization for RuBERT[1]."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Audio Automatic Speech Recognition", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "microsoft/wavlm-large", "api_call": "Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')", "api_arguments": "speech input", "python_environment_requirements": "transformers", "example_code": "To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.", "performance": {"dataset": "SUPERB benchmark", "accuracy": "state-of-the-art performance"}, "description": "WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "google/vit-base-patch16-224-in21k", "api_call": "ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')", "api_arguments": {"pretrained_model_name_or_path": "google/vit-base-patch16-224-in21k"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "ImageNet-21k", "accuracy": "Refer to tables 2 and 5 of the original paper"}, "description": "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Engineering", "api_name": "microsoft/unixcoder-base", "api_call": "AutoModel.from_pretrained('microsoft/unixcoder-base')", "api_arguments": {"tokenizer": "AutoTokenizer.from_pretrained('microsoft/unixcoder-base')"}, "python_environment_requirements": {"transformers": "from transformers import AutoTokenizer, AutoModel"}, "example_code": "tokenizer = AutoTokenizer.from_pretrained('microsoft/unixcoder-base')\nmodel = AutoModel.from_pretrained('microsoft/unixcoder-base')", "performance": {"dataset": "Not specified", "accuracy": "Not specified"}, "description": "UniXcoder is a unified cross-modal pre-trained model that leverages multimodal data (i.e. code comment and AST) to pretrain code representation. Developed by Microsoft Team and shared by Hugging Face. It is based on the RoBERTa model and trained on English language data. The model can be used for feature engineering tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Natural Language Processing Question Answering", "framework": "Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dpr-question_encoder-single-nq-base", "api_call": "DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')", "api_arguments": ["input_ids"], "python_environment_requirements": ["transformers"], "example_code": "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\nmodel = DPRQuestionEncoder.from_pretrained(facebook/dpr-question_encoder-single-nq-base)\ninput_ids = tokenizer(Hello, is my dog cute ?, return_tensors=pt)[input_ids]\nembeddings = model(input_ids).pooler_output", "performance": {"dataset": [{"name": "NQ", "accuracy": {"top_20": 78.4, "top_100": 85.4}}, {"name": "TriviaQA", "accuracy": {"top_20": 79.4, "top_100": 85.0}}, {"name": "WQ", "accuracy": {"top_20": 73.2, "top_100": 81.4}}, {"name": "TREC", "accuracy": {"top_20": 79.8, "top_100": 89.1}}, {"name": "SQuAD", "accuracy": {"top_20": 63.2, "top_100": 77.2}}]}, "description": "Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. dpr-question_encoder-single-nq-base is the question encoder trained using the Natural Questions (NQ) dataset (Lee et al., 2019; Kwiatkowski et al., 2019)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Audio Spectrogram", "api_name": "audio-spectrogram-transformer", "api_call": "ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "One custom ast model for testing of HF repos"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "rasa/LaBSE", "api_call": "AutoModel.from_pretrained('rasa/LaBSE')", "api_arguments": "input_text", "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "LaBSE (Language-agnostic BERT Sentence Embedding) model for extracting sentence embeddings in multiple languages."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Natural Language Processing Sentence Similarity", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "sentence-transformers/distilbert-base-nli-mean-tokens", "api_call": "SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')", "api_arguments": ["sentences"], "python_environment_requirements": "pip install -U sentence-transformers", "example_code": "from sentence_transformers import SentenceTransformer\nsentences = [This is an example sentence, Each sentence is converted]\nmodel = SentenceTransformer('sentence-transformers/distilbert-base-nli-mean-tokens')\nembeddings = model.encode(sentences)\nprint(embeddings)", "performance": {"dataset": "https://seb.sbert.net", "accuracy": "Not provided"}, "description": "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Document-level embeddings of research papers", "api_name": "malteos/scincl", "api_call": "AutoModel.from_pretrained('malteos/scincl')", "api_arguments": {"tokenizer": "AutoTokenizer.from_pretrained('malteos/scincl')", "model": "AutoModel.from_pretrained('malteos/scincl')"}, "python_environment_requirements": {"transformers": "4.13.0"}, "example_code": "from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('malteos/scincl')\nmodel = AutoModel.from_pretrained('malteos/scincl')\n\npapers = [{'title': 'BERT', 'abstract': 'We introduce a new language representation model called BERT'},\n {'title': 'Attention is all you need', 'abstract': ' The dominant sequence transduction models are based on complex recurrent or convolutional neural networks'}]\n\ntitle_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\n\ninputs = tokenizer(title_abs, padding=True, truncation=True, return_tensors=pt, max_length=512)\n\nresult = model(**inputs)\n\nembeddings = result.last_hidden_state[:, 0, :]", "performance": {"dataset": "SciDocs", "accuracy": {"mag-f1": 81.2, "mesh-f1": 89.0, "co-view-map": 85.3, "co-view-ndcg": 92.2, "co-read-map": 87.7, "co-read-ndcg": 94.0, "cite-map": 93.6, "cite-ndcg": 97.4, "cocite-map": 91.7, "cocite-ndcg": 96.5, "recomm-ndcg": 54.3, "recomm-P@1": 19.6}}, "description": "SciNCL is a pre-trained BERT language model to generate document-level embeddings of research papers. It uses the citation graph neighborhood to generate samples for contrastive learning. Prior to the contrastive training, the model is initialized with weights from scibert-scivocab-uncased. The underlying citation embeddings are trained on the S2ORC citation graph."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "sberbank-ai/sbert_large_mt_nlu_ru", "api_call": "AutoModel.from_pretrained('sberbank-ai/sbert_large_mt_nlu_ru')", "api_arguments": ["sentences", "padding", "truncation", "max_length", "return_tensors"], "python_environment_requirements": ["transformers", "torch"], "example_code": "from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\n\n# Sentences we want sentence embeddings for sentences = ['?']\n\n# Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\nmodel = AutoModel.from_pretrained(sberbank-ai/sbert_large_mt_nlu_ru)\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=24, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])", "performance": {"dataset": "Russian SuperGLUE", "accuracy": "Not provided"}, "description": "BERT large model multitask (cased) for Sentence Embeddings in Russian language."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Natural Language Processing Sentence Similarity", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "setu4993/LaBSE", "api_call": "BertModel.from_pretrained('setu4993/LaBSE')", "api_arguments": ["english_sentences", "italian_sentences", "japanese_sentences"], "python_environment_requirements": ["torch", "transformers"], "example_code": "import torch\nfrom transformers import BertModel, BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained('setu4993/LaBSE')\nmodel = BertModel.from_pretrained('setu4993/LaBSE')\nmodel = model.eval()\nenglish_sentences = [\n 'dog',\n 'Puppies are nice.',\n 'I enjoy taking long walks along the beach with my dog.',\n]\nenglish_inputs = tokenizer(english_sentences, return_tensors='pt', padding=True)\nwith torch.no_grad():\n english_outputs = model(**english_inputs)\nenglish_embeddings = english_outputs.pooler_output", "performance": {"dataset": "CommonCrawl and Wikipedia", "accuracy": "Not Specified"}, "description": "Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Natural Language Processing Token Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "lanwuwei/BERTOverflow_stackoverflow_github", "api_call": "AutoModelForTokenClassification.from_pretrained('lanwuwei/BERTOverflow_stackoverflow_github')", "api_arguments": {"pretrained_model_name_or_path": "lanwuwei/BERTOverflow_stackoverflow_github"}, "python_environment_requirements": {"transformers": "*", "torch": "*"}, "example_code": "from transformers import *\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)\nmodel = AutoModelForTokenClassification.from_pretrained(lanwuwei/BERTOverflow_stackoverflow_github)", "performance": {"dataset": "StackOverflow's 10 year archive", "accuracy": "Not provided"}, "description": "BERT-base model pre-trained on 152 million sentences from the StackOverflow's 10 year archive. It can be used for code and named entity recognition in StackOverflow."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "microsoft/xclip-base-patch16-zero-shot", "api_call": "XClipModel.from_pretrained('microsoft/xclip-base-patch16-zero-shot')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "For code examples, we refer to the documentation.", "performance": {"dataset": [{"name": "HMDB-51", "accuracy": 44.6}, {"name": "UCF-101", "accuracy": 72.0}, {"name": "Kinetics-600", "accuracy": 65.2}]}, "description": "X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Generation", "api_name": "runwayml/stable-diffusion-v1-5", "api_call": "StableDiffusionPipeline.from_pretrained(runwayml/stable-diffusion-v1-5, torch_dtype=torch.float16)", "api_arguments": {"prompt": "a photo of an astronaut riding a horse on mars"}, "python_environment_requirements": {"diffusers": "from diffusers import StableDiffusionPipeline", "torch": "import torch"}, "example_code": {"model_id": "model_id = runwayml/stable-diffusion-v1-5", "pipe": "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)", "pipe_to_cuda": "pipe = pipe.to(cuda)", "prompt": "prompt = a photo of an astronaut riding a horse on mars", "image": "image = pipe(prompt).images[0]", "save_image": "image.save(astronaut_rides_horse.png)"}, "performance": {"dataset": "COCO2017", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/dragon-plus-context-encoder", "api_call": "AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')", "api_arguments": ["pretrained"], "python_environment_requirements": ["torch", "transformers"], "example_code": "import torch\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained('facebook/dragon-plus-query-encoder')\nquery_encoder = AutoModel.from_pretrained('facebook/dragon-plus-query-encoder')\ncontext_encoder = AutoModel.from_pretrained('facebook/dragon-plus-context-encoder')\nquery = 'Where was Marie Curie born?'\ncontexts = [\n  'Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.',\n  'Born in Paris on 15 May 1859, Pierre Curie was the son of EugÃ¨ne Curie, a doctor of French Catholic origin from Alsace.'\n]\nquery_input = tokenizer(query, return_tensors='pt')\nctx_input = tokenizer(contexts, padding=True, truncation=True, return_tensors='pt')\nquery_emb = query_encoder(query_input).last_hidden_state[:, 0, :]\nctx_emb = context_encoder(ctx_input).last_hidden_state[:, 0, :]\nscore1 = query_emb @ ctx_emb[0]\nscore2 = query_emb @ ctx_emb[1]", "performance": {"dataset": "MS MARCO", "accuracy": 39.0}, "description": "DRAGON+ is a BERT-base sized dense retriever initialized from RetroMAE and further trained on the data augmented from MS MARCO corpus, following the approach described in How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval. The associated GitHub repository is available here https://github.com/facebookresearch/dpr-scale/tree/main/dragon. We use asymmetric dual encoder, with two distinctly parameterized encoders."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Generation", "api_name": "CompVis/stable-diffusion-v1-4", "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')", "api_arguments": ["prompt"], "python_environment_requirements": ["diffusers", "transformers", "scipy"], "example_code": "import torch\nfrom diffusers import StableDiffusionPipeline\nmodel_id = CompVis/stable-diffusion-v1-4\ndevice = cuda\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)", "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "prompthero/openjourney", "api_call": "StableDiffusionPipeline.from_pretrained('prompthero/openjourney')", "api_arguments": {"prompt": "string"}, "python_environment_requirements": ["diffusers", "torch"], "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = prompthero/openjourney\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = retro serie of different cars with different colors and shapes, mdjrny-v4 style\nimage = pipe(prompt).images[0]\nimage.save(./retro_cars.png)", "performance": {"dataset": "Midjourney images", "accuracy": "Not specified"}, "description": "Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Image Generation", "api_name": "runwayml/stable-diffusion-inpainting", "api_call": "StableDiffusionInpaintPipeline.from_pretrained('runwayml/stable-diffusion-inpainting')", "api_arguments": {"prompt": "Text prompt", "image": "PIL image", "mask_image": "PIL image (mask)"}, "python_environment_requirements": {"diffusers": "from diffusers import StableDiffusionInpaintPipeline"}, "example_code": {"import_code": "from diffusers import StableDiffusionInpaintPipeline", "instantiate_code": "pipe = StableDiffusionInpaintPipeline.from_pretrained(runwayml/stable-diffusion-inpainting, revision=fp16, torch_dtype=torch.float16)", "generate_image_code": "image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]", "save_image_code": "image.save(./yellow_cat_on_park_bench.png)"}, "performance": {"dataset": {"name": "LAION-2B (en)", "accuracy": "Not optimized for FID scores"}}, "description": "Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Generation", "api_name": "stabilityai/stable-diffusion-2-1-base", "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1-base', scheduler=EulerDiscreteScheduler.from_pretrained(stabilityai/stable-diffusion-2-1-base, subfolder=scheduler))", "api_arguments": {"prompt": "a photo of an astronaut riding a horse on mars"}, "python_environment_requirements": ["diffusers", "transformers", "accelerate", "scipy", "safetensors"], "example_code": {"install_dependencies": "pip install diffusers transformers accelerate scipy safetensors", "code": "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\nmodel_id = stabilityai/stable-diffusion-2-1-base\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)"}, "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "hakurei/waifu-diffusion", "api_call": "StableDiffusionPipeline.from_pretrained('hakurei/waifu-diffusion')", "api_arguments": {"prompt": "text", "guidance_scale": "number"}, "python_environment_requirements": {"torch": "torch", "autocast": "from torch", "StableDiffusionPipeline": "from diffusers"}, "example_code": "import torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\n 'hakurei/waifu-diffusion',\n torch_dtype=torch.float32\n).to('cuda')\nprompt = 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\nwith autocast(cuda):\n image = pipe(prompt, guidance_scale=6)[sample][0] \nimage.save(test.png)", "performance": {"dataset": "high-quality anime images", "accuracy": "not available"}, "description": "waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "stabilityai/sd-vae-ft-mse", "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')", "api_arguments": {"model": "CompVis/stable-diffusion-v1-4", "vae": "AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)"}, "python_environment_requirements": ["diffusers"], "example_code": "from diffusers.models import AutoencoderKL\nfrom diffusers import StableDiffusionPipeline\nmodel = CompVis/stable-diffusion-v1-4\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)", "performance": {"dataset": [{"name": "COCO 2017 (256x256, val, 5000 images)", "accuracy": {"rFID": "4.70", "PSNR": "24.5 +/- 3.7", "SSIM": "0.71 +/- 0.13", "PSIM": "0.92 +/- 0.27"}}, {"name": "LAION-Aesthetics 5+ (256x256, subset, 10000 images)", "accuracy": {"rFID": "1.88", "PSNR": "27.3 +/- 4.7", "SSIM": "0.83 +/- 0.11", "PSIM": "0.65 +/- 0.34"}}]}, "description": "This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Generation", "api_name": "stabilityai/stable-diffusion-2-1", "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-1')", "api_arguments": {"prompt": "a photo of an astronaut riding a horse on mars"}, "python_environment_requirements": ["diffusers", "transformers", "accelerate", "scipy", "safetensors"], "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nmodel_id = stabilityai/stable-diffusion-2-1\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)", "performance": {"dataset": "COCO2017", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "Realistic_Vision_V1.4", "api_call": "pipeline('text-to-image', model=SG161222/Realistic_Vision_V1.4)", "api_arguments": {"prompt": "string", "negative_prompt": "string"}, "python_environment_requirements": ["transformers", "torch"], "example_code": "from transformers import pipeline\n\nmodel = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\n\nprompt = 'a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3'\nnegative_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck'\n\nresult = model(prompt, negative_prompt=negative_prompt)", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Image generation and modification based on text prompts", "api_name": "stabilityai/stable-diffusion-2-inpainting", "api_call": "StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting')", "api_arguments": ["prompt", "image", "mask_image"], "python_environment_requirements": ["diffusers", "transformers", "accelerate", "scipy", "safetensors"], "example_code": "from diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained('stabilityai/stable-diffusion-2-inpainting', torch_dtype=torch.float16)\n\npipe.to(cuda)\nprompt = Face of a yellow cat, high resolution, sitting on a park bench\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\nimage.save(./yellow_cat_on_park_bench.png)", "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "dreamlike-art/dreamlike-photoreal-2.0", "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-photoreal-2.0')", "api_arguments": {"prompt": "photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens"}, "python_environment_requirements": {"torch": "torch.float16", "diffusers": "StableDiffusionPipeline"}, "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-photoreal-2.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\nimage = pipe(prompt).images[0]\nimage.save(./result.jpg)", "performance": {"dataset": "Stable Diffusion 1.5", "accuracy": "Not specified"}, "description": "Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Generation", "api_name": "stabilityai/stable-diffusion-2", "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2', subfolder=scheduler))", "api_arguments": {"prompt": "a photo of an astronaut riding a horse on mars"}, "python_environment_requirements": ["diffusers", "transformers", "accelerate", "scipy", "safetensors"], "example_code": "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nmodel_id = stabilityai/stable-diffusion-2\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=scheduler)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)", "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "andite/anything-v4.0", "api_call": "StableDiffusionPipeline.from_pretrained('andite/anything-v4.0')", "api_arguments": {"model_id": "andite/anything-v4.0", "torch_dtype": "torch.float16", "device": "cuda", "prompt": "hatsune_miku"}, "python_environment_requirements": {"diffusers": "StableDiffusionPipeline", "torch": "torch"}, "example_code": {"from diffusers import StableDiffusionPipeline": "", "import torch": "", "model_id = andite/anything-v4.0": "", "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)": "", "pipe = pipe.to(cuda)": "", "prompt = hatsune_miku": "", "image = pipe(prompt).images[0]": "", "image.save(./hatsune_miku.png)": ""}, "performance": {"dataset": "Not specified", "accuracy": "Not specified"}, "description": "Anything V4 is a latent diffusion model for generating high-quality, highly detailed anime-style images with just a few prompts. It supports danbooru tags to generate images and can be used just like any other Stable Diffusion model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "prompthero/openjourney-v4", "api_call": "pipeline('text-to-image', model='prompthero/openjourney-v4')", "api_arguments": {"text": "string"}, "python_environment_requirements": ["transformers"], "example_code": "generate_image('your text here')", "performance": {"dataset": "Midjourney v4 images", "accuracy": "Not provided"}, "description": "Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "stabilityai/sd-vae-ft-ema", "api_call": "StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae=AutoencoderKL.from_pretrained('stabilityai/sd-vae-ft-ema'))", "api_arguments": {"model": "CompVis/stable-diffusion-v1-4", "vae": "AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)"}, "python_environment_requirements": {"diffusers": "diffusers library"}, "example_code": "from diffusers.models import AutoencoderKL\nfrom diffusers import StableDiffusionPipeline\nmodel = CompVis/stable-diffusion-v1-4\nvae = AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-ema)\npipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)", "performance": {"dataset": {"COCO 2017 (256x256, val, 5000 images)": {"accuracy": {"rFID": 4.42, "PSNR": "23.8 +/- 3.9", "SSIM": "0.69 +/- 0.13", "PSIM": "0.96 +/- 0.27"}}, "LAION-Aesthetics 5+ (256x256, subset, 10000 images)": {"accuracy": {"rFID": 1.77, "PSNR": "26.7 +/- 4.8", "SSIM": "0.82 +/- 0.12", "PSIM": "0.67 +/- 0.34"}}}}, "description": "This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Generate and modify images based on text prompts", "api_name": "stabilityai/stable-diffusion-2-depth", "api_call": "StableDiffusionDepth2ImgPipeline.from_pretrained('stabilityai/stable-diffusion-2-depth')", "api_arguments": {"prompt": "Text prompt to generate image", "image": "Initial image (optional)", "negative_prompt": "Negative text prompt to avoid certain features", "strength": "Strength of the prompt effect on the generated image"}, "python_environment_requirements": ["pip install -U git+https://github.com/huggingface/transformers.git", "pip install diffusers transformers accelerate scipy safetensors"], "example_code": "import torch\nimport requests\nfrom PIL import Image\nfrom diffusers import StableDiffusionDepth2ImgPipeline\n\npipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n stabilityai/stable-diffusion-2-depth,\n torch_dtype=torch.float16,\n).to(cuda)\n\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\ninit_image = Image.open(requests.get(url, stream=True).raw)\nprompt = two tigers\nn_propmt = bad, deformed, ugly, bad anotomy\nimage = pipe(prompt=prompt, image=init_image, negative_prompt=n_propmt, strength=0.7).images[0]", "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "EimisAnimeDiffusion_1.0v", "api_call": "DiffusionPipeline.from_pretrained('eimiss/EimisAnimeDiffusion_1.0v')", "api_arguments": "['prompt']", "python_environment_requirements": "huggingface_hub", "example_code": "from huggingface_hub import hf_hub_download; hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', 'prompt')", "performance": {"dataset": "Not specified", "accuracy": "Not specified"}, "description": "EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image generation", "api_name": "stabilityai/stable-diffusion-2-base", "api_call": "StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', scheduler=EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler))", "api_arguments": {"prompt": "a photo of an astronaut riding a horse on mars"}, "python_environment_requirements": ["diffusers", "transformers", "accelerate", "scipy", "safetensors"], "example_code": "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\nmodel_id = stabilityai/stable-diffusion-2-base\nscheduler = EulerDiscreteScheduler.from_pretrained('stabilityai/stable-diffusion-2-base', subfolder=scheduler)\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = a photo of an astronaut riding a horse on mars\nimage = pipe(prompt).images[0]\nimage.save(astronaut_rides_horse.png)", "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "nitrosocke/nitro-diffusion", "api_call": "StableDiffusionPipeline.from_pretrained('nitrosocke/nitro-diffusion')", "api_arguments": ["prompt"], "python_environment_requirements": ["torch", "diffusers"], "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = nitrosocke/nitro-diffusion\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = archer arcane style magical princess with golden hair\nimage = pipe(prompt).images[0]\nimage.save(./magical_princess.png)", "performance": {"dataset": "Stable Diffusion", "accuracy": "N/A"}, "description": "Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "Linaqruf/anything-v3.0", "api_call": "Text2ImagePipeline(model='Linaqruf/anything-v3.0')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A text-to-image model that generates images from text descriptions."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "wavymulder/Analog-Diffusion", "api_call": "pipeline('text-to-image', model='wavymulder/Analog-Diffusion')", "api_arguments": ["prompt"], "python_environment_requirements": ["transformers"], "example_code": "text_to_image('analog style landscape')", "performance": {"dataset": "analog photographs", "accuracy": "Not specified"}, "description": "Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "dreamlike-art/dreamlike-diffusion-1.0", "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-diffusion-1.0')", "api_arguments": ["prompt"], "python_environment_requirements": ["diffusers", "torch"], "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-diffusion-1.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors\nimage = pipe(prompt).images[0]\nimage.save(./result.jpg)", "performance": {"dataset": "high quality art", "accuracy": "not provided"}, "description": "Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "dreamlike-art/dreamlike-anime-1.0", "api_call": "StableDiffusionPipeline.from_pretrained('dreamlike-art/dreamlike-anime-1.0')", "api_arguments": ["prompt", "negative_prompt"], "python_environment_requirements": ["diffusers", "torch"], "example_code": "from diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = dreamlike-art/dreamlike-anime-1.0\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(cuda)\nprompt = anime, masterpiece, high quality, 1girl, solo, long hair, looking at viewer, blush, smile, bangs, blue eyes, skirt, medium breasts, iridescent, gradient, colorful, besides a cottage, in the country\nnegative_prompt = 'simple background, duplicate, retro style, low quality, lowest quality, 1980s, 1990s, 2000s, 2005 2006 2007 2008 2009 2010 2011 2012 2013, bad anatomy, bad proportions, extra digits, lowres, username, artist name, error, duplicate, watermark, signature, text, extra digit, fewer digits, worst quality, jpeg artifacts, blurry'\nimage = pipe(prompt, negative_prompt=negative_prompt).images[0]\nimage.save(./result.jpg)", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "Lykon/DreamShaper", "api_call": "pipeline('text-to-image', model=Lykon/DreamShaper)", "api_arguments": "", "python_environment_requirements": "transformers, torch", "example_code": "https://huggingface.co/spaces/Lykon/DreamShaper-webui", "performance": {"dataset": "", "accuracy": ""}, "description": "Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "darkstorm2150/Protogen_v2.2_Official_Release", "api_call": "StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v2.2_Official_Release')", "api_arguments": {"model_id": "darkstorm2150/Protogen_v2.2_Official_Release", "torch_dtype": "torch.float16"}, "python_environment_requirements": {"diffusers": "StableDiffusionPipeline, DPMSolverMultistepScheduler", "torch": "torch"}, "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\nprompt = (\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\n)\nmodel_id = darkstorm2150/Protogen_v2.2_Official_Release\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage.save(./result.jpg)", "performance": {"dataset": "Various datasets", "accuracy": "Not specified"}, "description": "Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "gsdf/Counterfeit-V2.5", "api_call": "pipeline('text-to-image', model='gsdf/Counterfeit-V2.5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "((masterpiece,best quality)),1girl, solo, animal ears, rabbit, barefoot, knees up, dress, sitting, rabbit ears, short sleeves, looking at viewer, grass, short hair, smile, white hair, puffy sleeves, outdoors, puffy short sleeves, bangs, on ground, full body, animal, white dress, sunlight, brown eyes, dappled sunlight, day, depth of field", "performance": {"dataset": "EasyNegative", "accuracy": "Not provided"}, "description": "Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "vintedois-diffusion-v0-1", "api_call": "pipeline('text-to-image', model='22h/vintedois-diffusion-v0-1')", "api_arguments": ["prompt", "CFG Scale", "Scheduler", "Steps", "Seed"], "python_environment_requirements": ["transformers"], "example_code": "text2img('photo of an old man in a jungle, looking at the camera', CFG Scale=7.5, Scheduler='diffusers.EulerAncestralDiscreteScheduler', Steps=30, Seed=44)", "performance": {"dataset": "large amount of high quality images", "accuracy": "not specified"}, "description": "Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Image generation and modification based on text prompts", "api_name": "stabilityai/stable-diffusion-x4-upscaler", "api_call": "StableDiffusionUpscalePipeline.from_pretrained('stabilityai/stable-diffusion-x4-upscaler')", "api_arguments": {"model_id": "stabilityai/stable-diffusion-x4-upscaler", "torch_dtype": "torch.float16"}, "python_environment_requirements": ["diffusers", "transformers", "accelerate", "scipy", "safetensors", "xformers (optional, for memory efficient attention)"], "example_code": "pip install diffusers transformers accelerate scipy safetensors\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom diffusers import StableDiffusionUpscalePipeline\nimport torch\n\nmodel_id = stabilityai/stable-diffusion-x4-upscaler\npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipeline = pipeline.to(cuda)\n\nurl = https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\nresponse = requests.get(url)\nlow_res_img = Image.open(BytesIO(response.content)).convert(RGB)\nlow_res_img = low_res_img.resize((128, 128))\nprompt = a white cat\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\nupscaled_image.save(upsampled_cat.png)", "performance": {"dataset": "COCO2017 validation set", "accuracy": "Not optimized for FID scores"}, "description": "Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "darkstorm2150/Protogen_x5.8_Official_Release", "api_call": "StableDiffusionPipeline.from_pretrained('darkstorm2150/Protogen_v5.8_Official_Release')", "api_arguments": {"model_id": "darkstorm2150/Protogen_v5.8_Official_Release", "torch_dtype": "torch.float16"}, "python_environment_requirements": ["torch", "diffusers"], "example_code": "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\nimport torch\nprompt = (\nmodelshoot style, (extremely detailed CG unity 8k wallpaper), full shot body photo of the most beautiful artwork in the world, \nenglish medieval witch, black silk vale, pale skin, black silk robe, black cat, necromancy magic, medieval era, \nphotorealistic painting by Ed Blinkey, Atey Ghailan, Studio Ghibli, by Jeremy Mann, Greg Manchess, Antonio Moro, trending on ArtStation, \ntrending on CGSociety, Intricate, High Detail, Sharp focus, dramatic, photorealistic painting art by midjourney and greg rutkowski\n)\nmodel_id = darkstorm2150/Protogen_v5.8_Official_Release\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(cuda)\nimage = pipe(prompt, num_inference_steps=25).images[0]\nimage.save(./result.jpg)", "performance": {"dataset": "unknown", "accuracy": "unknown"}, "description": "Protogen x5.8 is a text-to-image model that generates images based on text prompts. It was warm-started with Stable Diffusion v1-5 and is rebuilt using dreamlikePhotoRealV2.ckpt as a core. The model uses granular adaptive learning techniques for fine-grained adjustments and can be used just like any other Stable Diffusion model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "nlpconnect/vit-gpt2-image-captioning", "api_call": "VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')", "api_arguments": {"model": "nlpconnect/vit-gpt2-image-captioning"}, "python_environment_requirements": ["transformers", "torch", "PIL"], "example_code": "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\nmodel = VisionEncoderDecoderModel.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\nfeature_extractor = ViTImageProcessor.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ntokenizer = AutoTokenizer.from_pretrained(nlpconnect/vit-gpt2-image-captioning)\ndevice = torch.device(cuda if torch.cuda.is_available() else cpu)\nmodel.to(device)\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {max_length: max_length, num_beams: num_beams}\ndef predict_step(image_paths):\n images = []\n for image_path in image_paths:\n i_image = Image.open(image_path)\n if i_image.mode != RGB:\n i_image = i_image.convert(mode=RGB)\nimages.append(i_image)\npixel_values = feature_extractor(images=images, return_tensors=pt).pixel_values\n pixel_values = pixel_values.to(device)\noutput_ids = model.generate(pixel_values, **gen_kwargs)\npreds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\npredict_step(['doctor.e16ba4e4.jpg']) # ['a woman in a hospital bed with a woman in a hospital bed']", "performance": {"dataset": "Not provided", "accuracy": "Not provided"}, "description": "An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Image Upscaling", "api_name": "stabilityai/sd-x2-latent-upscaler", "api_call": "StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler)", "api_arguments": {"prompt": "text prompt", "image": "low resolution latents", "num_inference_steps": 20, "guidance_scale": 0, "generator": "torch generator"}, "python_environment_requirements": ["git+https://github.com/huggingface/diffusers.git", "transformers", "accelerate", "scipy", "safetensors"], "example_code": "from diffusers import StableDiffusionLatentUpscalePipeline, StableDiffusionPipeline\nimport torch\npipeline = StableDiffusionPipeline.from_pretrained(CompVis/stable-diffusion-v1-4, torch_dtype=torch.float16)\npipeline.to(cuda)\nupscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(stabilityai/sd-x2-latent-upscaler, torch_dtype=torch.float16)\nupscaler.to(cuda)\nprompt = a photo of an astronaut high resolution, unreal engine, ultra realistic\ngenerator = torch.manual_seed(33)\nlow_res_latents = pipeline(prompt, generator=generator, output_type=latent).images\nupscaled_image = upscaler(prompt=prompt, image=low_res_latents, num_inference_steps=20, guidance_scale=0, generator=generator).images[0]\nupscaled_image.save(astronaut_1024.png)", "performance": {"dataset": "LAION-2B", "accuracy": "Not specified"}, "description": "Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "kha-white/manga-ocr-base", "api_call": "pipeline('ocr', model='kha-white/manga-ocr-base')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "manga109s", "accuracy": ""}, "description": "Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "blip-image-captioning-base", "api_call": "BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base')", "api_arguments": ["raw_image", "text", "return_tensors"], "python_environment_requirements": ["requests", "PIL", "transformers"], "example_code": "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-base)\nmodel = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-base)\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\ntext = a photography of\ninputs = processor(raw_image, text, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))", "performance": {"dataset": "COCO", "accuracy": {"CIDEr": "+2.8%"}}, "description": "BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Transformers", "functionality": "Image Captioning", "api_name": "blip-image-captioning-large", "api_call": "BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)", "api_arguments": {"raw_image": "Image", "text": "Optional Text"}, "python_environment_requirements": {"transformers": "BlipProcessor, BlipForConditionalGeneration", "PIL": "Image", "requests": "requests"}, "example_code": {"import_requests": "import requests", "import_PIL": "from PIL import Image", "import_transformers": "from transformers import BlipProcessor, BlipForConditionalGeneration", "load_processor": "processor = BlipProcessor.from_pretrained(Salesforce/blip-image-captioning-large)", "load_model": "model = BlipForConditionalGeneration.from_pretrained(Salesforce/blip-image-captioning-large)", "load_image": "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')", "conditional_captioning": "text = a photography of\ninputs = processor(raw_image, text, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))", "unconditional_captioning": "inputs = processor(raw_image, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))"}, "performance": {"dataset": "COCO", "accuracy": {"image-text retrieval": "+2.7% recall@1", "image captioning": "+2.8% CIDEr", "VQA": "+1.6% VQA score"}}, "description": "BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-base-printed", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')", "api_arguments": ["images", "return_tensors"], "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-printed')\npixel_values = processor(images=image, return_tensors=pt).pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]", "performance": {"dataset": "SROIE", "accuracy": "Not provided"}, "description": "TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "blip2-opt-2.7b", "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')", "api_arguments": {"img_url": "https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg", "question": "how many dogs are in the picture?"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": {"import_requests": "import requests", "import_PIL": "from PIL import Image", "import_transformers": "from transformers import BlipProcessor, Blip2ForConditionalGeneration", "load_processor": "processor = BlipProcessor.from_pretrained('Salesforce/blip2-opt-2.7b')", "load_model": "model = Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-opt-2.7b')", "load_image": "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')", "process_inputs": "inputs = processor(raw_image, question, return_tensors='pt')", "generate_output": "out = model.generate(**inputs)", "decode_output": "print(processor.decode(out[0], skip_special_tokens=True))"}, "performance": {"dataset": "LAION", "accuracy": "Not specified"}, "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-small-handwritten", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')", "api_arguments": ["images", "return_tensors"], "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\npixel_values = processor(images=image, return_tensors='pt').pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]", "performance": {"dataset": "IAM", "accuracy": "Not provided"}, "description": "TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "naver-clova-ix/donut-base", "api_call": "AutoModel.from_pretrained('naver-clova-ix/donut-base')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "result = donut(image_path)", "performance": {"dataset": "arxiv:2111.15664", "accuracy": "Not provided"}, "description": "Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "promptcap-coco-vqa", "api_call": "PromptCap('vqascore/promptcap-coco-vqa')", "api_arguments": {"prompt": "string", "image": "string"}, "python_environment_requirements": "pip install promptcap", "example_code": ["import torch", "from promptcap import PromptCap", "model = PromptCap(vqascore/promptcap-coco-vqa)", "if torch.cuda.is_available():", "  model.cuda()", "prompt = please describe this image according to the given question: what piece of clothing is this boy putting on?", "image = glove_boy.jpeg", "print(model.caption(prompt, image))"], "performance": {"dataset": {"coco": {"accuracy": "150 CIDEr"}, "OK-VQA": {"accuracy": "60.4%"}, "A-OKVQA": {"accuracy": "59.6%"}}}, "description": "PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/git-base-coco", "api_call": "pipeline('text-generation', model='microsoft/git-base-coco')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "See the model hub for fine-tuned versions on a task that interests you.", "performance": {"dataset": "COCO", "accuracy": "Refer to the paper for evaluation results."}, "description": "GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "AICVTG_What_if_a_machine_could_create_captions_automatically", "api_call": "VisionEncoderDecoderModel.from_pretrained('facebook/mmt-en-de')", "api_arguments": {"image_paths": "List of image file paths", "max_length": 20, "num_beams": 8}, "python_environment_requirements": {"transformers": "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer", "torch": "import torch", "Image": "from PIL import Image"}, "example_code": "predict_step(['Image URL.jpg'])", "performance": {"dataset": "Not specified", "accuracy": "Not specified"}, "description": "This is an image captioning model training by Zayn"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "blip2-flan-t5-xl", "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xl')", "api_arguments": ["raw_image", "question"], "python_environment_requirements": ["transformers", "requests", "PIL"], "example_code": ["import requests", "from PIL import Image", "from transformers import BlipProcessor, Blip2ForConditionalGeneration", "processor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xl)", "model = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xl)", "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'", "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')", "question = how many dogs are in the picture?", "inputs = processor(raw_image, question, return_tensors=pt)", "out = model.generate(**inputs)", "print(processor.decode(out[0], skip_special_tokens=True))"], "performance": {"dataset": "LAION", "accuracy": "Not provided"}, "description": "BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "blip2-flan-t5-xxl", "api_call": "Blip2ForConditionalGeneration.from_pretrained('Salesforce/blip2-flan-t5-xxl')", "api_arguments": {"raw_image": "Image", "question": "Text"}, "python_environment_requirements": ["requests", "PIL", "transformers"], "example_code": "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, Blip2ForConditionalGeneration\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip2-flan-t5-xxl)\nmodel = Blip2ForConditionalGeneration.from_pretrained(Salesforce/blip2-flan-t5-xxl)\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = how many dogs are in the picture?\ninputs = processor(raw_image, question, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))", "performance": {"dataset": "LAION", "accuracy": "Not provided"}, "description": "BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-large-handwritten", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')", "api_arguments": {"pretrained_model_name_or_path": "microsoft/trocr-large-handwritten"}, "python_environment_requirements": {"packages": ["transformers", "PIL", "requests"]}, "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')\npixel_values = processor(images=image, return_tensors=pt).pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]", "performance": {"dataset": "IAM", "accuracy": "Not specified"}, "description": "TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image-to-Text", "api_name": "ydshieh/vit-gpt2-coco-en", "api_call": "VisionEncoderDecoderModel.from_pretrained('ydshieh/vit-gpt2-coco-en')", "api_arguments": {"loc": "ydshieh/vit-gpt2-coco-en"}, "python_environment_requirements": ["torch", "requests", "PIL", "transformers"], "example_code": "import torch\nimport requests\nfrom PIL import Image\nfrom transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\nloc = ydshieh/vit-gpt2-coco-en\nfeature_extractor = ViTFeatureExtractor.from_pretrained(loc)\ntokenizer = AutoTokenizer.from_pretrained(loc)\nmodel = VisionEncoderDecoderModel.from_pretrained(loc)\nmodel.eval()\ndef predict(image):\n pixel_values = feature_extractor(images=image, return_tensors=pt).pixel_values\n with torch.no_grad():\n  output_ids = model.generate(pixel_values, max_length=16, num_beams=4, return_dict_in_generate=True).sequences\n preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n preds = [pred.strip() for pred in preds]\n return preds\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nwith Image.open(requests.get(url, stream=True).raw) as image:\n preds = predict(image)\nprint(preds)", "performance": {"dataset": "COCO", "accuracy": "Not specified"}, "description": "A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "text2text-generation", "api_name": "blip2-opt-6.7b", "api_call": "pipeline('text2text-generation', model='salesforce/blip2-opt-6.7b')", "api_arguments": "image, optional text", "python_environment_requirements": "transformers", "example_code": "Refer to the documentation", "performance": {"dataset": "LAION", "accuracy": "Not specified"}, "description": "BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-base-handwritten", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')", "api_arguments": {"pretrained_model_name_or_path": "microsoft/trocr-base-handwritten"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\npixel_values = processor(images=image, return_tensors='pt').pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]", "performance": {"dataset": "IAM", "accuracy": "Not specified"}, "description": "TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "donut-base-finetuned-cord-v2", "api_call": "AutoModel.from_pretrained('naver-clova-ix/donut-base-finetuned-cord-v2')", "api_arguments": {"image": "path_to_image"}, "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; image_to_text = pipeline('image-to-text', model='naver-clova-ix/donut-base-finetuned-cord-v2'); image_to_text('path_to_image')", "performance": {"dataset": "CORD", "accuracy": "Not provided"}, "description": "Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-coco", "api_call": "GenerativeImage2TextModel.from_pretrained('microsoft/git-large-coco')", "api_arguments": "image, text", "python_environment_requirements": "transformers", "example_code": "For code examples, we refer to the documentation.", "performance": {"dataset": "COCO", "accuracy": "See table 11 in the paper for more details."}, "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/pix2struct-chartqa-base", "api_call": "Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-chartqa-base')", "api_arguments": ["t5x_checkpoint_path", "pytorch_dump_path", "use-large"], "python_environment_requirements": "transformers", "example_code": "python convert_pix2struct_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --pytorch_dump_path PATH_TO_SAVE", "performance": {"dataset": "ChartQA", "accuracy": "Not provided"}, "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/pix2struct-base", "api_call": "Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-base')", "api_arguments": {"t5x_checkpoint_path": "PATH_TO_T5X_CHECKPOINTS", "pytorch_dump_path": "PATH_TO_SAVE"}, "python_environment_requirements": {"transformers": "4.15.0", "torch": "1.10.1"}, "example_code": "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nmodel = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\nprocessor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\nmodel.push_to_hub(USERNAME/MODEL_NAME)\nprocessor.push_to_hub(USERNAME/MODEL_NAME)", "performance": {"dataset": [{"name": "Documents", "accuracy": "N/A"}, {"name": "Illustrations", "accuracy": "N/A"}, {"name": "User Interfaces", "accuracy": "N/A"}, {"name": "Natural Images", "accuracy": "N/A"}]}, "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/pix2struct-textcaps-base", "api_call": "Pix2StructForConditionalGeneration.from_pretrained('google/pix2struct-textcaps-base')", "api_arguments": {"images": "image", "text": "text", "return_tensors": "pt", "max_patches": 512}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": ["import requests", "from PIL import Image", "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor", "url = https://www.ilankelman.org/stopsigns/australia.jpg", "image = Image.open(requests.get(url, stream=True).raw)", "model = Pix2StructForConditionalGeneration.from_pretrained(google/pix2struct-textcaps-base)", "processor = Pix2StructProcessor.from_pretrained(google/pix2struct-textcaps-base)", "inputs = processor(images=image, return_tensors=pt)", "predictions = model.generate(**inputs)", "print(processor.decode(predictions[0], skip_special_tokens=True))"], "performance": {"dataset": "TextCaps", "accuracy": "state-of-the-art"}, "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "microsoft/git-base", "api_call": "pipeline('image-to-text', model='microsoft/git-base')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "git_base(image)", "performance": {"dataset": ["COCO", "Conceptual Captions (CC3M)", "SBU", "Visual Genome (VG)", "Conceptual Captions (CC12M)", "ALT200M"], "accuracy": "Refer to the paper for evaluation results"}, "description": "GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-large-printed", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')", "api_arguments": {"TrOCRProcessor": "from_pretrained('microsoft/trocr-large-printed')", "images": "image", "return_tensors": "pt"}, "python_environment_requirements": {"transformers": "pip install transformers", "PIL": "pip install pillow", "requests": "pip install requests"}, "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-printed')\npixel_values = processor(images=image, return_tensors=pt).pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]", "performance": {"dataset": "SROIE", "accuracy": "Not provided"}, "description": "TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/deplot", "api_call": "Pix2StructForConditionalGeneration.from_pretrained('google/deplot')", "api_arguments": {"images": "image", "text": "question", "return_tensors": "pt", "max_new_tokens": 512}, "python_environment_requirements": {"transformers": "Pix2StructForConditionalGeneration, Pix2StructProcessor", "requests": "requests", "PIL": "Image"}, "example_code": "from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\nimport requests\nfrom PIL import Image\nmodel = Pix2StructForConditionalGeneration.from_pretrained('google/deplot')\nprocessor = Pix2StructProcessor.from_pretrained('google/deplot')\nurl = https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, text=Generate underlying data table of the figure below:, return_tensors=pt)\npredictions = model.generate(**inputs, max_new_tokens=512)\nprint(processor.decode(predictions[0], skip_special_tokens=True))", "performance": {"dataset": "ChartQA", "accuracy": "24.0% improvement over finetuned SOTA"}, "description": "DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-textcaps", "api_call": "AutoModelForCausalLM.from_pretrained('microsoft/git-large-textcaps')", "api_arguments": "image, text", "python_environment_requirements": "transformers", "example_code": "N/A", "performance": {"dataset": "TextCaps", "accuracy": "Refer to the paper"}, "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-r-textcaps", "api_call": "pipeline('text-generation', model='microsoft/git-large-r-textcaps')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "TextCaps", "accuracy": ""}, "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-small-stage1", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')", "api_arguments": {"url": "https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg"}, "python_environment_requirements": ["transformers", "PIL", "requests", "torch"], "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\nimport torch\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\n\npixel_values = processor(image, return_tensors='pt').pixel_values\ndecoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\noutputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)", "performance": {"dataset": "IAM", "accuracy": "Not provided"}, "description": "TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-small-printed", "api_call": "VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')", "api_arguments": {"image": "Image.open(requests.get(url, stream=True).raw).convert('RGB')", "processor": "TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom PIL import Image\nimport requests\n\nurl = 'https://fki.tic.heia-fr.ch/static/img/a01-122-02-00.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')\nprocessor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\nmodel = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\npixel_values = processor(images=image, return_tensors='pt').pixel_values\ngenerated_ids = model.generate(pixel_values)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]", "performance": {"dataset": "SROIE", "accuracy": "Not specified"}, "description": "TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video Synthesis", "api_name": "modelscope-damo-text-to-video-synthesis", "api_call": "pipeline('text-to-video-synthesis')", "api_arguments": {"text": "A short text description in English"}, "python_environment_requirements": ["modelscope==1.4.2", "open_clip_torch", "pytorch-lightning"], "example_code": "from huggingface_hub import snapshot_download\nfrom modelscope.pipelines import pipeline\nfrom modelscope.outputs import OutputKeys\nimport pathlib\n\nmodel_dir = pathlib.Path('weights')\nsnapshot_download('damo-vilab/modelscope-damo-text-to-video-synthesis',\n repo_type='model', local_dir=model_dir)\n\npipe = pipeline('text-to-video-synthesis', model_dir.as_posix())\n\ntest_text = {\n 'text': 'A panda eating bamboo on a rock.',\n}\n\noutput_video_path = pipe(test_text,)[OutputKeys.OUTPUT_VIDEO]\nprint('output_video_path:', output_video_path)", "performance": {"dataset": "Webvid, ImageNet, LAION5B", "accuracy": "Not provided"}, "description": "This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "mgp-str", "api_call": "MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')", "api_arguments": {"model_name": "alibaba-damo/mgp-str-base"}, "python_environment_requirements": {"packages": ["transformers"]}, "example_code": "from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\nimport requests\nfrom PIL import Image\nprocessor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\nmodel = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\nurl = https://i.postimg.cc/ZKwLg2Gw/367-14.png\nimage = Image.open(requests.get(url, stream=True).raw).convert(RGB)\npixel_values = processor(images=image, return_tensors=pt).pixel_values\noutputs = model(pixel_values)\ngenerated_text = processor.batch_decode(outputs.logits)['generated_text']", "performance": {"dataset": "MJSynth and SynthText", "accuracy": null}, "description": "MGP-STR is a pure vision Scene Text Recognition (STR) model, consisting of ViT and specially designed A^3 modules. It is trained on MJSynth and SynthText datasets and can be used for optical character recognition (OCR) on text images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-video synthesis", "api_name": "damo-vilab/text-to-video-ms-1.7b", "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')", "api_arguments": {"torch_dtype": "torch.float16", "variant": "fp16"}, "python_environment_requirements": "pip install diffusers transformers accelerate", "example_code": "import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)", "performance": {"dataset": "Webvid, ImageNet, LAION5B", "accuracy": "N/A"}, "description": "This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "ImRma/Brucelee", "api_call": "pipeline('text-to-video', model='ImRma/Brucelee')", "api_arguments": ["your_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Hugging Face model for converting Persian and English text into video."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "camenduru/text2-video-zero", "api_call": "pipeline('text-to-video', model='camenduru/text2-video-zero')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video Synthesis", "api_name": "damo-vilab/text-to-video-ms-1.7b-legacy", "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b-legacy')", "api_arguments": ["prompt", "num_inference_steps"], "python_environment_requirements": ["diffusers", "transformers", "accelerate"], "example_code": "import torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\npipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b-legacy, torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)", "performance": {"dataset": ["LAION5B", "ImageNet", "Webvid"], "accuracy": "Not provided"}, "description": "This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-video-synthesis", "api_name": "damo-vilab/text-to-video-ms-1.7b", "api_call": "DiffusionPipeline.from_pretrained('damo-vilab/text-to-video-ms-1.7b')", "api_arguments": ["prompt", "num_inference_steps", "num_frames"], "python_environment_requirements": ["pip install git+https://github.com/huggingface/diffusers transformers accelerate"], "example_code": "pipe = DiffusionPipeline.from_pretrained(damo-vilab/text-to-video-ms-1.7b, torch_dtype=torch.float16, variant=fp16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\nprompt = Spiderman is surfing\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)", "performance": {"dataset": "Webvid", "accuracy": "Not specified"}, "description": "A multi-stage text-to-video generation diffusion model that inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. It supports English input only and has a wide range of applications."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "duncan93/video", "api_call": "BaseModel.from_pretrained('duncan93/video')", "api_arguments": "", "python_environment_requirements": "Asteroid", "example_code": "", "performance": {"dataset": "OpenAssistant/oasst1", "accuracy": ""}, "description": "A text-to-video model trained on OpenAssistant/oasst1 dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video Generation", "api_name": "mo-di-bear-guitar", "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/mo-di-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/mo-di-bear-guitar', subfolder='unet'), torch_dtype=torch.float16)", "api_arguments": {"prompt": "string", "video_length": "int", "height": "int", "width": "int", "num_inference_steps": "int", "guidance_scale": "float"}, "python_environment_requirements": ["torch", "tuneavideo"], "example_code": "from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\nfrom tuneavideo.models.unet import UNet3DConditionModel\nfrom tuneavideo.util import save_videos_grid\nimport torch\npretrained_model_path = nitrosocke/mo-di-diffusion\nunet_model_path = Tune-A-Video-library/mo-di-bear-guitar\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\npipe.enable_xformers_memory_efficient_attention()\nprompt = a magical princess is playing guitar, modern disney style\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\nsave_videos_grid(video, f./{prompt}.gif)", "performance": {"dataset": "Not mentioned", "accuracy": "Not mentioned"}, "description": "Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video Generation", "api_name": "redshift-man-skiing", "api_call": "TuneAVideoPipeline.from_pretrained('nitrosocke/redshift-diffusion', unet=UNet3DConditionModel.from_pretrained('Tune-A-Video-library/redshift-man-skiing', subfolder='unet'))", "api_arguments": {"prompt": "string", "video_length": "int", "height": "int", "width": "int", "num_inference_steps": "int", "guidance_scale": "float"}, "python_environment_requirements": ["torch", "tuneavideo"], "example_code": "from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline\nfrom tuneavideo.models.unet import UNet3DConditionModel\nfrom tuneavideo.util import save_videos_grid\nimport torch\npretrained_model_path = nitrosocke/redshift-diffusion\nunet_model_path = Tune-A-Video-library/redshift-man-skiing\nunet = UNet3DConditionModel.from_pretrained(unet_model_path, subfolder='unet', torch_dtype=torch.float16).to('cuda')\npipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(cuda)\npipe.enable_xformers_memory_efficient_attention()\nprompt = (redshift style) spider man is skiing\nvideo = pipe(prompt, video_length=8, height=512, width=512, num_inference_steps=50, guidance_scale=7.5).videos\nsave_videos_grid(video, f./{prompt}.gif)", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/git-base-textvqa", "api_call": "AutoModel.from_pretrained('microsoft/git-base-textvqa')", "api_arguments": "image, question", "python_environment_requirements": "transformers", "example_code": "vqa_pipeline({'image': 'path/to/image.jpg', 'question': 'What is in the image?'})", "performance": {"dataset": "TextVQA", "accuracy": "Refer to the paper"}, "description": "GIT (GenerativeImage2Text), base-sized, fine-tuned on TextVQA. It is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is to predict the next text token, giving the image tokens and previous text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/git-base-vqav2", "api_call": "pipeline('visual-question-answering', model='microsoft/git-base-vqav2')", "api_arguments": "image, question", "python_environment_requirements": ["transformers"], "example_code": "vqa(image='path/to/image.jpg', question='What is in the image?')", "performance": {"dataset": "VQAv2", "accuracy": "Refer to the paper for evaluation results"}, "description": "GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "ivelin/donut-refexp-combined-v1", "api_call": "pipeline('visual-question-answering', model='ivelin/donut-refexp-combined-v1')", "api_arguments": "image, question", "python_environment_requirements": "transformers", "example_code": "vqa(image='path/to/image.jpg', question='What is the color of the object?')", "performance": {"dataset": "ivelin/donut-refexp-combined-v1", "accuracy": "N/A"}, "description": "A visual question answering model that takes an image and a question as input and provides an answer based on the visual content of the image and the context of the question."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/git-large-vqav2", "api_call": "AutoModel.from_pretrained('microsoft/git-large-vqav2')", "api_arguments": {"model": "microsoft/git-large-vqav2", "task": "visual-question-answering", "device": 0}, "python_environment_requirements": ["transformers"], "example_code": "from transformers import pipeline; vqa_pipeline = pipeline('visual-question-answering', model='microsoft/git-large-vqav2', device=0); results = vqa_pipeline({'image': 'path_to_image', 'question': 'your_question'})", "performance": {"dataset": "VQAv2", "accuracy": "Refer to the paper"}, "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "dandelin/vilt-b32-finetuned-vqa", "api_call": "ViltForQuestionAnswering.from_pretrained('dandelin/vilt-b32-finetuned-vqa')", "api_arguments": {"image": "Image.open(requests.get(url, stream=True).raw)", "text": "How many cats are there?"}, "python_environment_requirements": {"transformers": "ViltProcessor, ViltForQuestionAnswering", "requests": "requests", "PIL": "Image"}, "example_code": "from transformers import ViltProcessor, ViltForQuestionAnswering\nimport requests\nfrom PIL import Image\n\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = How many cats are there?\nprocessor = ViltProcessor.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\nmodel = ViltForQuestionAnswering.from_pretrained(dandelin/vilt-b32-finetuned-vqa)\n\nencoding = processor(image, text, return_tensors=pt)\noutputs = model(**encoding)\nlogits = outputs.logits\nidx = logits.argmax(-1).item()\nprint(Predicted answer:, model.config.id2label[idx])", "performance": {"dataset": "VQAv2", "accuracy": "to do"}, "description": "Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Kim et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Visual Question Answering", "api_name": "blip-vqa-base", "api_call": "BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base')", "api_arguments": {"raw_image": "Image", "question": "String"}, "python_environment_requirements": {"transformers": "BlipProcessor, BlipForQuestionAnswering", "PIL": "Image", "requests": "requests"}, "example_code": "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-base)\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-base)\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = how many dogs are in the picture?\ninputs = processor(raw_image, question, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))", "performance": {"dataset": "VQA", "accuracy": "+1.6% in VQA score"}, "description": "BLIP is a Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is trained on visual question answering with a base architecture (using ViT base backbone)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Visual Question Answering", "api_name": "Salesforce/blip-vqa-capfilt-large", "api_call": "BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-capfilt-large')", "api_arguments": {"raw_image": "RGB image", "question": "string"}, "python_environment_requirements": {"transformers": "BlipProcessor, BlipForQuestionAnswering"}, "example_code": "import requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nprocessor = BlipProcessor.from_pretrained(Salesforce/blip-vqa-capfilt-large)\nmodel = BlipForQuestionAnswering.from_pretrained(Salesforce/blip-vqa-capfilt-large)\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = how many dogs are in the picture?\ninputs = processor(raw_image, question, return_tensors=pt)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))", "performance": {"dataset": "VQA", "accuracy": "+1.6% in VQA score"}, "description": "BLIP is a new Vision-Language Pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. The model achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-textvqa", "api_call": "AutoModelForSeq2SeqLM.from_pretrained('microsoft/git-large-textvqa')", "api_arguments": "image, question", "python_environment_requirements": "transformers", "example_code": "For code examples, we refer to the documentation.", "performance": {"dataset": "TextVQA", "accuracy": "See table 11 in the paper for more details."}, "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextVQA. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like: image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "hf-tiny-model-private/tiny-random-ViltForQuestionAnswering", "api_call": "ViltForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-ViltForQuestionAnswering')", "api_arguments": {"image": "path/to/image/file", "question": "your_question"}, "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random model for Visual Question Answering using the VILT framework."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "azwierzc/vilt-b32-finetuned-vqa-pl", "api_call": "pipeline('visual-question-answering', model='azwierzc/vilt-b32-finetuned-vqa-pl')", "api_arguments": {"image": "path_to_image", "question": "question_text"}, "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Visual Question Answering model fine-tuned on the Polish language."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face", "functionality": "Visual Question Answering", "api_name": "sheldonxxxx/OFA_model_weights", "api_call": "AutoModel.from_pretrained('sheldonxxxx/OFA_model_weights')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "This is an unoffical mirror of the model weights for use with https://github.com/OFA-Sys/OFA. The original link is too slow when downloading from outside of China."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "vilt-finetuned-vqasi", "api_call": "ViltModel.from_pretrained('tufa15nik/vilt-finetuned-vqasi')", "api_arguments": {"model": "tufa15nik/vilt-finetuned-vqasi", "tokenizer": "tufa15nik/vilt-finetuned-vqasi"}, "python_environment_requirements": {"transformers": ">=4.11.3"}, "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Visual Question Answering model fine-tuned on the VQASI dataset by tufa15nik using the ViLT architecture. The model is designed to answer questions based on the content of an input image."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face", "functionality": "Visual Question Answering", "api_name": "JosephusCheung/GuanacoVQA", "api_call": "pipeline('visual-question-answering', model='GuanacoVQA').", "api_arguments": "N/A", "python_environment_requirements": "transformers, torch", "example_code": "N/A", "performance": {"dataset": "JosephusCheung/GuanacoVQADataset", "accuracy": "N/A"}, "description": "A multilingual Visual Question Answering model supporting English, Chinese, Japanese, and German languages. It requires the combined use of the Guanaco 7B LLM model and is based on the implementation of MiniGPT-4."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Visual Question Answering", "api_name": "temp_vilt_vqa", "api_call": "pipeline('visual-question-answering', model='Bingsu/temp_vilt_vqa', tokenizer='Bingsu/temp_vilt_vqa')", "api_arguments": {"model": "Bingsu/temp_vilt_vqa", "tokenizer": "Bingsu/temp_vilt_vqa"}, "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A visual question answering model for answering questions related to images using the Hugging Face Transformers library."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face", "functionality": "Visual Question Answering", "api_name": "JosephusCheung/GuanacoVQAOnConsumerHardware", "api_call": "pipeline('visual-question-answering', model='JosephusCheung/GuanacoVQAOnConsumerHardware')", "api_arguments": {"model": "JosephusCheung/GuanacoVQAOnConsumerHardware", "tokenizer": "JosephusCheung/GuanacoVQAOnConsumerHardware"}, "python_environment_requirements": {"transformers": "latest", "torch": "latest"}, "example_code": "vqa(image_path, question)", "performance": {"dataset": "JosephusCheung/GuanacoVQADataset", "accuracy": "unknown"}, "description": "A Visual Question Answering model trained on the GuanacoVQADataset, designed to work on consumer hardware like Colab Free T4 GPU. The model can be used to answer questions about images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "impira/layoutlm-document-qa", "api_call": "pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))", "api_arguments": ["image_url", "question"], "python_environment_requirements": ["PIL", "pytesseract", "PyTorch", "transformers"], "example_code": "nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)", "performance": {"dataset": ["SQuAD2.0", "DocVQA"], "accuracy": "Not provided"}, "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "layoutlmv2-base-uncased-finetuned-docvqa", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')", "api_arguments": [], "python_environment_requirements": ["transformers==4.12.2", "torch==1.8.0+cu101", "datasets==1.14.0", "tokenizers==0.10.3"], "example_code": "", "performance": {"dataset": "unknown", "accuracy": {"Loss": 1.194}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "xhyi/layoutlmv3_docvqa_t11c5000", "api_call": "pipeline('question-answering', model='xhyi/layoutlmv3_docvqa_t11c5000')", "api_arguments": "question, context", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "DocVQA", "accuracy": ""}, "description": "LayoutLMv3 model trained for document question answering task."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Question Answering", "api_name": "impira/layoutlm-invoices", "api_call": "pipeline('question-answering', model='impira/layoutlm-invoices')", "api_arguments": "question, context", "python_environment_requirements": "transformers", "example_code": "qa_pipeline(question='your question', context='your document context')", "performance": {"dataset": "proprietary dataset of invoices, SQuAD2.0, and DocVQA", "accuracy": "not provided"}, "description": "This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional classifier head."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "dperales/layoutlmv2-base-uncased_finetuned_docvqa", "api_call": "LayoutLMv2ForQuestionAnswering.from_pretrained('dperales/layoutlmv2-base-uncased_finetuned_docvqa')", "api_arguments": {"model": "dperales/layoutlmv2-base-uncased_finetuned_docvqa"}, "python_environment_requirements": {"transformers": "latest"}, "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A model for Document Question Answering based on the LayoutLMv2 architecture, fine-tuned on the DocVQA dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "vision-encoder-decoder", "api_name": "naver-clova-ix/donut-base-finetuned-docvqa", "api_call": "pipeline('document-question-answering', model='donut-base-finetuned-docvqa')", "api_arguments": {"image": "path_to_image", "question": "your_question"}, "python_environment_requirements": "Transformers", "example_code": "from transformers import pipeline\n\n# Initialize the pipeline\ndoc_qa = pipeline('document-question-answering', model='naver-clova-ix/donut-base-finetuned-docvqa')\n\n# Load an image and ask a question\nimage_path = 'path_to_image'\nquestion = 'your_question'\n\n# Get the answer\nanswer = doc_qa({'image': image_path, 'question': question})\nprint(answer)", "performance": {"dataset": "DocVQA", "accuracy": "Not provided"}, "description": "Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "CZ_DVQA_layoutxlm-base", "api_call": "LayoutXLMForQuestionAnswering.from_pretrained('fimu-docproc-research/CZ_DVQA_layoutxlm-base')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Document Question Answering model based on LayoutXLM."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "layoutlm-vqa", "api_call": "pipeline('question-answering', model='pardeepSF/layoutlm-vqa')", "api_arguments": {"question": "string", "context": "string"}, "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A model for document question answering using the LayoutLM architecture."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "layoutlm-invoices", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('impira/layoutlm-invoices')", "api_arguments": "question, context", "python_environment_requirements": "transformers", "example_code": "nlp(question='What is the total amount?', context='your_invoice_text')", "performance": {"dataset": "proprietary dataset of invoices, SQuAD2.0, and DocVQA", "accuracy": "Not provided"}, "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of invoices as well as both SQuAD2.0 and DocVQA for general comprehension. Unlike other QA models, which can only extract consecutive tokens, this model can predict longer-range, non-consecutive sequences with an additional classifier head."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "layoutlmv3-base-mpdocvqa", "api_call": "LayoutLMv3ForQuestionAnswering.from_pretrained('rubentito/layoutlmv3-base-mpdocvqa')", "api_arguments": ["image", "question", "context", "boxes"], "python_environment_requirements": ["torch", "transformers"], "example_code": "import torch\nfrom transformers import LayoutLMv3Processor, LayoutLMv3ForQuestionAnswering\nprocessor = LayoutLMv3Processor.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa, apply_ocr=False)\nmodel = LayoutLMv3ForQuestionAnswering.from_pretrained(rubentito/layoutlmv3-base-mpdocvqa)\nimage = Image.open(example.jpg).convert(RGB)\nquestion = Is this a question?\ncontext = [Example]\nboxes = [0, 0, 1000, 1000]\ndocument_encoding = processor(image, question, context, boxes=boxes, return_tensors=pt)\noutputs = model(**document_encoding)\nstart_idx = torch.argmax(outputs.start_logits, axis=1)\nend_idx = torch.argmax(outputs.end_logits, axis=1)\nanswers = self.processor.tokenizer.decode(input_tokens[start_idx: end_idx+1]).strip()", "performance": {"dataset": "rubentito/mp-docvqa", "accuracy": {"ANLS": 0.4538, "APPA": 51.9426}}, "description": "This is pretrained LayoutLMv3 from Microsoft hub and fine-tuned on Multipage DocVQA (MP-DocVQA) dataset. This model was used as a baseline in Hierarchical multimodal transformers for Multi-Page DocVQA."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "vision-encoder-decoder", "api_name": "jinhybr/OCR-DocVQA-Donut", "api_call": "pipeline('document-question-answering', model='jinhybr/OCR-DocVQA-Donut')", "api_arguments": "image_path, question", "python_environment_requirements": "transformers", "example_code": "doc_vqa(image_path='path/to/image.jpg', question='What is the title?')", "performance": {"dataset": "DocVQA", "accuracy": "Not provided"}, "description": "Donut model fine-tuned on DocVQA. It consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings, after which the decoder autoregressively generates text, conditioned on the encoding of the encoder."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Transformers", "functionality": "Document Question Answering", "api_name": "tiny-random-LayoutLMv3ForQuestionAnswering", "api_call": "LayoutLMv3ForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMv3ForQuestionAnswering')", "api_arguments": {"image": "path/to/image/file"}, "python_environment_requirements": ["transformers", "torch", "tensorflow"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random LayoutLMv3 model for document question answering. Can be used with the Hugging Face Inference API."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "DataIntelligenceTeam/eurocorpV4", "api_call": "AutoModelForTokenClassification.from_pretrained('DataIntelligenceTeam/eurocorpV4')", "api_arguments": "", "python_environment_requirements": "transformers>=4.26.0.dev0, torch>=1.12.1+cu113, datasets>=2.2.2, tokenizers>=0.13.2", "example_code": "", "performance": {"dataset": "sroie", "accuracy": 0.982}, "description": "This model is a fine-tuned version of microsoft/layoutlmv3-large on the sroie dataset. It achieves the following results on the evaluation set: Loss: 0.1239, Precision: 0.9548, Recall: 0.9602, F1: 0.9575, Accuracy: 0.9819"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Document Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "frizwankhan/entity-linking-model-final", "api_call": "pipeline('question-answering', model='frizwankhan/entity-linking-model-final')", "api_arguments": {"image": "path/to/image", "question": "your question"}, "python_environment_requirements": {"huggingface": "4.12.0", "torch": "1.9.0"}, "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Document Question Answering model based on layoutlmv2"}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "seungwon12/layoutlmv2-base-uncased_finetuned_docvqa", "api_call": "pipeline('question-answering', model='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa', tokenizer='seungwon12/layoutlmv2-base-uncased_finetuned_docvqa')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "DocVQA", "accuracy": ""}, "description": "A document question answering model finetuned on the DocVQA dataset using LayoutLMv2-base-uncased."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "LayoutLMX_pt_question_answer_ocrazure_correct_V16_07_04_2023", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')", "api_arguments": {"question": "string", "context": "string"}, "python_environment_requirements": {"transformers": ">=4.0.0"}, "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A LayoutLMv2 model for document question answering."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "layoutlmv2-base-uncased_finetuned_docvqa", "api_call": "pipeline('question-answering', model='Sayantan1993/layoutlmv2-base-uncased_finetuned_docvqa')", "api_arguments": "{'question': 'your_question', 'context': 'your_context'}", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A model for document question answering, fine-tuned on the DocVQA dataset using LayoutLMv2-base-uncased."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "layoutlmv2-base-uncased-finetuned-infovqa", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-infovqa')", "api_arguments": {}, "python_environment_requirements": {"transformers": "4.12.2", "pytorch": "1.8.0+cu101", "datasets": "1.14.0", "tokenizers": "0.10.3"}, "example_code": "", "performance": {"dataset": "unknown", "accuracy": {"Loss": 2.087}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "layoutlmv2-base-uncased_finetuned_docvqa", "api_call": "AutoModel.from_pretrained('microsoft/layoutlmv2-base-uncased')", "api_arguments": {"model": "hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa"}, "python_environment_requirements": {"transformers": "4.27.4", "pytorch": "2.0.0+cu117", "datasets": "2.11.0", "tokenizers": "0.13.2"}, "example_code": "from transformers import pipeline\nqa_pipeline = pipeline('document-question-answering', model='hugginglaoda/layoutlmv2-base-uncased_finetuned_docvqa')\nquestion = 'What is the total amount?'\ndocument = 'path/to/your/image/file.png'\nresult = qa_pipeline(question=question, document=document)\nprint(result)", "performance": {"dataset": "None", "accuracy": {"Loss": 4.843}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-large-uncased-finetuned-infovqa')", "api_arguments": "question, context", "python_environment_requirements": "transformers==4.12.3, Pytorch==1.8.0+cu101, Datasets==1.15.1, Tokenizers==0.10.3", "example_code": "", "performance": {"dataset": "unknown", "accuracy": {"Loss": 2.2207}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V15_30_03_2023')", "api_arguments": {}, "python_environment_requirements": {"transformers": ">=4.11.0"}, "example_code": {}, "performance": {"dataset": {}, "accuracy": {}}, "description": "A document question answering model based on LayoutLMv2, which can be used to extract answers from images with text and layout information."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('L-oenai/LayoutLMX_pt_question_answer_ocrazure_correct_V18_08_04_2023')", "api_arguments": {"question": "string", "context": "string"}, "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A LayoutLM model for document question answering."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "layoutlmv2-base-uncased_finetuned_docvqa", "api_call": "AutoModelForDocumentQuestionAnswering.from_pretrained('tiennvcs/layoutlmv2-base-uncased-finetuned-docvqa')", "api_arguments": "question, image", "python_environment_requirements": "transformers, torch, datasets, tokenizers", "example_code": "", "performance": {"dataset": "None", "accuracy": {"Loss": 4.3167}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on the None dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Transformers", "functionality": "Document Question Answering", "api_name": "tiny-random-LayoutLMForQuestionAnswering", "api_call": "AutoModelForQuestionAnswering.from_pretrained('hf-tiny-model-private/tiny-random-LayoutLMForQuestionAnswering')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random LayoutLM model for question answering. This model is not pretrained and serves as an example for the LayoutLM architecture."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face", "functionality": "Question Answering", "api_name": "impira/layoutlm-document-qa", "api_call": "pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('impira/layoutlm-document-qa'))", "api_arguments": ["image_url", "question"], "python_environment_requirements": ["PIL", "pytesseract", "PyTorch", "transformers"], "example_code": "nlp(https://templates.invoicehome.com/invoice-template-us-neat-750px.png, What is the invoice number?)", "performance": {"dataset": "SQuAD2.0 and DocVQA", "accuracy": "Not provided"}, "description": "A fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa", "api_call": "pipeline('question-answering', model='tiennvcs/layoutlmv2-base-uncased-finetuned-vi-infovqa')", "api_arguments": "question, context", "python_environment_requirements": ["transformers==4.15.0", "torch==1.8.0+cu101", "datasets==1.17.0", "tokenizers==0.10.3"], "example_code": "", "performance": {"dataset": "unknown", "accuracy": {"Loss": 4.3332}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-base-uncased on an unknown dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa", "api_call": "pipeline('question-answering', model='tiennvcs/layoutlmv2-large-uncased-finetuned-vi-infovqa')", "api_arguments": "", "python_environment_requirements": "transformers==4.15.0, torch==1.8.0+cu101, datasets==1.17.0, tokenizers==0.10.3", "example_code": "", "performance": {"dataset": "unknown", "accuracy": {"Loss": 8.5806}}, "description": "This model is a fine-tuned version of microsoft/layoutlmv2-large-uncased on an unknown dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Multimodal Graph Machine Learning", "framework": "Hugging Face Transformers", "functionality": "Graph Classification", "api_name": "graphormer-base-pcqm4mv2", "api_call": "AutoModel.from_pretrained('clefourrier/graphormer-base-pcqm4mv2')", "api_arguments": "pretrained_model_name", "python_environment_requirements": "transformers", "example_code": "See the Graph Classification with Transformers tutorial.", "performance": {"dataset": "PCQM4M-LSCv2", "accuracy": "Not provided"}, "description": "The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2. Developed by Microsoft, it is designed for graph classification tasks or graph representation tasks, such as molecule modeling."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Graph Machine Learning", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "graphormer-base-pcqm4mv1", "api_call": "AutoModel.from_pretrained('graphormer-base-pcqm4mv1')", "api_arguments": ["model_name"], "python_environment_requirements": ["transformers"], "example_code": "See the Graph Classification with Transformers tutorial", "performance": {"dataset": "PCQM4M-LSC", "accuracy": "1st place on the KDD CUP 2021 (quantum prediction track)"}, "description": "The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSC, and which got 1st place on the KDD CUP 2021 (quantum prediction track). Developed by Microsoft, this model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Multimodal Document Question Answer", "framework": "Hugging Face Transformers", "functionality": "Document Question Answering", "api_name": "CQI_Visual_Question_Awnser_PT_v0", "api_call": "pipeline('question-answering', model=LayoutLMForQuestionAnswering.from_pretrained('microsoft/layoutlm-base-uncased'))", "api_arguments": ["url", "question"], "python_environment_requirements": ["PIL", "pytesseract", "PyTorch", "transformers"], "example_code": ["nlp('https://templates.invoicehome.com/invoice-template-us-neat-750px.png', 'What is the invoice number?')", "nlp('https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg', 'What is the purchase amount?')", "nlp('https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png', 'What are the 2020 net sales?')"], "performance": {"dataset": [{"accuracy": 0.9943977}, {"accuracy": 0.9912159}, {"accuracy": 0.59147286}]}, "description": "A model for visual question answering in Portuguese and English, capable of processing PDFs and images to extract information and answer questions."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "dpt-large-redesign", "api_call": "AutoModelForDepthEstimation.from_pretrained('nielsr/dpt-large-redesign')", "api_arguments": [], "python_environment_requirements": ["torch", "transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A depth estimation model based on the DPT architecture."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "hf-tiny-model-private/tiny-random-GLPNForDepthEstimation", "api_call": "AutoModel.from_pretrained('hf-tiny-model-private/tiny-random-GLPNForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["torch", "transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random GLPN model for depth estimation using the Hugging Face Transformers library."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-kitti", "api_call": "GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-kitti')", "api_arguments": "images, return_tensors", "python_environment_requirements": "transformers", "example_code": "from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-kitti)\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-kitti)\ninputs = feature_extractor(images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n predicted_depth = outputs.predicted_depth\nprediction = torch.nn.functional.interpolate(\n predicted_depth.unsqueeze(1),\n size=image.size[::-1],\n mode=bicubic,\n align_corners=False,\n)\noutput = prediction.squeeze().cpu().numpy()\nformatted = (output * 255 / np.max(output)).astype(uint8)\ndepth = Image.fromarray(formatted)", "performance": {"dataset": "KITTI", "accuracy": "Not provided"}, "description": "Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-230131-041708", "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')", "api_arguments": "", "python_environment_requirements": "Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.4425, "Mae": 0.427, "Rmse": 0.6196, "Abs_Rel": 0.4543, "Log_Mae": 0.1732, "Log_Rmse": 0.2288, "Delta1": 0.3787, "Delta2": 0.6298, "Delta3": 0.8083}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Monocular Depth Estimation", "api_name": "Intel/dpt-large", "api_call": "DPTForDepthEstimation.from_pretrained('Intel/dpt-large')", "api_arguments": {"pretrained_model_name_or_path": "Intel/dpt-large"}, "python_environment_requirements": ["transformers"], "example_code": "from transformers import DPTImageProcessor, DPTForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = DPTImageProcessor.from_pretrained(Intel/dpt-large)\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-large)\ninputs = processor(images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n predicted_depth = outputs.predicted_depth\nprediction = torch.nn.functional.interpolate(\n predicted_depth.unsqueeze(1),\n size=image.size[::-1],\n mode=bicubic,\n align_corners=False,\n)\noutput = prediction.squeeze().cpu().numpy()\nformatted = (output * 255 / np.max(output)).astype(uint8)\ndepth = Image.fromarray(formatted)", "performance": {"dataset": "MIX 6", "accuracy": "10.82"}, "description": "Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu", "api_call": "GLPNForDepthEstimation.from_pretrained('vinvino02/glpn-nyu')", "api_arguments": "images, return_tensors", "python_environment_requirements": ["transformers", "torch", "numpy", "PIL", "requests"], "example_code": "from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = GLPNFeatureExtractor.from_pretrained(vinvino02/glpn-nyu)\nmodel = GLPNForDepthEstimation.from_pretrained(vinvino02/glpn-nyu)\ninputs = feature_extractor(images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n predicted_depth = outputs.predicted_depth\nprediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=bicubic, align_corners=False,)\noutput = prediction.squeeze().cpu().numpy()\nformatted = (output * 255 / np.max(output)).astype(uint8)\ndepth = Image.fromarray(formatted)", "performance": {"dataset": "NYUv2", "accuracy": "Not provided"}, "description": "Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu-finetuned-diode", "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.4359, "Rmse": 0.4276}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "Intel/dpt-hybrid-midas", "api_call": "DPTForDepthEstimation.from_pretrained('Intel/dpt-hybrid-midas', low_cpu_mem_usage=True)", "api_arguments": {"pretrained_model_name_or_path": "Intel/dpt-hybrid-midas", "low_cpu_mem_usage": "True"}, "python_environment_requirements": ["torch", "transformers", "PIL", "numpy", "requests"], "example_code": "from PIL import Image\nimport numpy as np\nimport requests\nimport torch\nfrom transformers import DPTForDepthEstimation, DPTFeatureExtractor\nmodel = DPTForDepthEstimation.from_pretrained(Intel/dpt-hybrid-midas, low_cpu_mem_usage=True)\nfeature_extractor = DPTFeatureExtractor.from_pretrained(Intel/dpt-hybrid-midas)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\nwith torch.no_grad():\n outputs = model(**inputs)\n predicted_depth = outputs.predicted_depth\nprediction = torch.nn.functional.interpolate(\n predicted_depth.unsqueeze(1),\n size=image.size[::-1],\n mode=bicubic,\n align_corners=False,\n)\noutput = prediction.squeeze().cpu().numpy()\nformatted = (output * 255 / np.max(output)).astype(uint8)\ndepth = Image.fromarray(formatted)\ndepth.show()", "performance": {"dataset": "MIX 6", "accuracy": "11.06"}, "description": "Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221215-093747", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-093747')", "api_arguments": [], "python_environment_requirements": ["transformers", "torch"], "example_code": "", "performance": {"dataset": "DIODE", "accuracy": ""}, "description": "A depth estimation model fine-tuned on the DIODE dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221215-092352", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-092352')", "api_arguments": {}, "python_environment_requirements": {"huggingface_transformers": "4.13.0"}, "example_code": "", "performance": {"dataset": "DIODE", "accuracy": ""}, "description": "A depth estimation model fine-tuned on the DIODE dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221215-095508", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-095508')", "api_arguments": null, "python_environment_requirements": ["transformers"], "example_code": null, "performance": {"dataset": "DIODE", "accuracy": null}, "description": "A depth estimation model fine-tuned on the DIODE dataset using the GLPN model architecture."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221215-112116", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221215-112116')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "DIODE", "accuracy": ""}, "description": "A depth estimation model fine-tuned on the DIODE dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221122-030603", "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-030603')", "api_arguments": [], "python_environment_requirements": ["transformers==4.24.0", "torch==1.12.1"], "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3597, "Mae": 0.3054, "Rmse": 0.4481, "Abs Rel": 0.3462, "Log Mae": 0.1256, "Log Rmse": 0.1798, "Delta1": 0.5278, "Delta2": 0.8055, "Delta3": 0.9191}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-kitti-finetuned-diode", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-kitti-finetuned-diode')", "api_arguments": "N/A", "python_environment_requirements": "transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2", "example_code": "N/A", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.5845, "Rmse": 0.6175}}, "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu-finetuned-diode-221116-054332", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-054332')", "api_arguments": {"model_name": "sayakpaul/glpn-nyu-finetuned-diode-221116-054332"}, "python_environment_requirements": {"transformers": "4.24.0", "pytorch": "1.13.0+cu117", "tokenizers": "0.13.2"}, "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.6028, "Rmse": "nan"}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Multimodal Graph Machine Learning", "framework": "Hugging Face Transformers", "functionality": "GTA5 AI model", "api_name": "GTA5_PROCESS_LEARNING_AI", "api_call": "AutoModelForSeq2SeqLM.from_pretrained('janpase97/codeformer-pretrained')", "api_arguments": {"model": "NanoCircuit", "data_loader": "train_loader", "criterion": "nn.CrossEntropyLoss", "optimizer": "optim.SGD", "device": "torch.device", "data_cap_gb": 10}, "python_environment_requirements": ["contextlib", "os", "matplotlib", "numpy", "torch", "torch.nn", "torch.optim", "requests", "torchvision", "psutil", "time", "subprocess", "onnxruntime", "numexpr", "transformers"], "example_code": {"import_libraries": ["import contextlib", "import os", "from matplotlib import pyplot as plt", "import numpy as np", "import torch", "import torch.nn as nn", "import torch.optim as optim", "import requests", "from torchvision import datasets, transforms", "import psutil", "import time", "import subprocess", "import onnxruntime as ort", "import matplotlib.pyplot as plt", "import numpy as np", "import numexpr as ne", "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"], "define_neural_network": ["class NanoCircuit(nn.Module):", " def init(self):", " super(NanoCircuit, self).init()", " self.fc1 = nn.Linear(784, 128)", " self.fc2 = nn.Linear(128, 10)", "def forward(self, x):", " x = x.view(-1, 784)", " x = torch.relu(self.fc1(x))", " x = self.fc2(x)", " return x"], "train_with_data_cap": ["def train_with_data_cap(model, data_loader, criterion, optimizer, device, data_cap_gb):", " data_processed = 0", " data_cap_bytes = data_cap_gb * (1024 ** 3)", " epoch = 0", "while data_processed < data_cap_bytes:", " running_loss = 0.0", " for i, data in enumerate(data_loader, 0):", " inputs, labels = data", " inputs, labels = inputs.to(device), labels.to(device)", " data_processed += inputs.nelement() * inputs.element_size()", " if data_processed >= data_cap_bytes:", " break", " optimizer.zero_grad()", " outputs = model(inputs.view(-1, 28 * 28))", " loss = criterion(outputs, labels)", " loss.backward()", " optimizer.step()", " running_loss += loss.item()", "epoch += 1", "print(fEpoch {epoch}, Loss: {running_loss / (i + 1)})", "print(fData processed: {data_processed / (1024 ** 3):.2f} GB)", "return model"]}, "performance": {"dataset": "MNIST", "accuracy": "Not specified"}, "description": "This AI model is designed to train on the MNIST dataset with a specified data cap and save the trained model as an .onnx file. It can be attached to the GTA5 game process by PID and checks if the targeted application is running. The model is trained on a GPU if available."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu-finetuned-diode-221116-062619", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-062619')", "api_arguments": "None", "python_environment_requirements": "Transformers 4.24.0, Pytorch 1.13.0+cu117, Tokenizers 0.13.2", "example_code": "None", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.548, "Rmse": "nan"}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu-finetuned-diode-221116-104421", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-104421')", "api_arguments": "", "python_environment_requirements": "transformers==4.24.0, pytorch==1.12.1+cu113, tokenizers==0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3736, "Mae": 0.3079, "Rmse": 0.4321, "Abs Rel": 0.3666, "Log Mae": 0.1288, "Log Rmse": 0.1794, "Delta1": 0.4929, "Delta2": 0.7934, "Delta3": 0.9234}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221121-063504", "api_call": "AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221121-063504')", "api_arguments": [], "python_environment_requirements": ["transformers==4.24.0", "torch==1.12.1+cu116", "tokenizers==0.13.2"], "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3533, "Mae": 0.2668, "Rmse": 0.3716, "Abs Rel": 0.3427, "Log Mae": 0.1167, "Log Rmse": 0.1703, "Delta1": 0.5522, "Delta2": 0.8362, "Delta3": 0.9382}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset for depth estimation."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221116-110652", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221116-110652')", "api_arguments": "", "python_environment_requirements": "transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.4018, "Mae": 0.3272, "Rmse": 0.4546, "Abs Rel": 0.3934, "Log Mae": 0.138, "Log Rmse": 0.1907, "Delta1": 0.4598, "Delta2": 0.7659, "Delta3": 0.9082}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221121-113853", "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221121-113853')", "api_arguments": [], "python_environment_requirements": ["transformers==4.24.0", "torch==1.12.1", "tokenizers==0.13.2"], "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3384, "Mae": 0.2739, "Rmse": 0.3959, "Abs Rel": 0.323, "Log Mae": 0.1148, "Log Rmse": 0.1651, "Delta1": 0.5576, "Delta2": 0.8345, "Delta3": 0.9398}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221122-014502", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-014502')", "api_arguments": "", "python_environment_requirements": "transformers==4.24.0, pytorch==1.12.1+cu116, tokenizers==0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3476, "Mae": 0.2763, "Rmse": 0.4088, "Abs Rel": 0.3308, "Log Mae": 0.1161, "Log Rmse": 0.17, "Delta1": 0.5682, "Delta2": 0.8301, "Delta3": 0.9279}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It achieves depth estimation with various performance metrics."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221122-044810", "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221122-044810')", "api_arguments": "", "python_environment_requirements": "transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.369, "Mae": 0.2909, "Rmse": 0.4208, "Abs Rel": 0.3635, "Log Mae": 0.1224, "Log Rmse": 0.1793, "Delta1": 0.5323, "Delta2": 0.8179, "Delta3": 0.9258}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221122-082237", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221122-082237')", "api_arguments": "pretrained_model_name", "python_environment_requirements": "transformers>=4.24.0, pytorch>=1.12.1, tokenizers>=0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3421, "Mae": 0.27, "Rmse": 0.4042, "Abs Rel": 0.3279, "Log Mae": 0.1132, "Log Rmse": 0.1688, "Delta1": 0.5839, "Delta2": 0.8408, "Delta3": 0.9309}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-kitti-finetuned-diode-221214-123047", "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-kitti-finetuned-diode-221214-123047')", "api_arguments": [], "python_environment_requirements": ["transformers==4.24.0", "torch==1.12.1+cu116", "tokenizers==0.13.2"], "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.3497, "Mae": 0.2847, "Rmse": 0.3977, "Abs Rel": 0.3477, "Log Mae": 0.1203, "Log Rmse": 0.1726, "Delta1": 0.5217, "Delta2": 0.8246, "Delta3": 0.9436}}, "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221221-102136", "api_call": "pipeline('depth-estimation', model='sayakpaul/glpn-nyu-finetuned-diode-221221-102136')", "api_arguments": [], "python_environment_requirements": ["Transformers 4.24.0", "Pytorch 1.12.1+cu116", "Datasets 2.8.0", "Tokenizers 0.13.2"], "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.4222, "Mae": 0.411, "Rmse": 0.6292, "Abs Rel": 0.3778, "Log Mae": 0.1636, "Log Rmse": 0.224, "Delta1": 0.432, "Delta2": 0.6806, "Delta3": 0.8068}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221228-072509", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-221228-072509')", "api_arguments": "", "python_environment_requirements": "Transformers 4.24.0, Pytorch 1.12.1+cu116, Datasets 2.8.0, Tokenizers 0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.4012, "Mae": 0.403, "Rmse": 0.6173, "Abs Rel": 0.3487, "Log Mae": 0.1574, "Log Rmse": 0.211, "Delta1": 0.4308, "Delta2": 0.6997, "Delta3": 0.8249}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu-finetuned-diode-230103-091356", "api_call": "AutoModel.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230103-091356')", "api_arguments": "", "python_environment_requirements": "transformers==4.24.0, pytorch==1.12.1+cu116, datasets==2.8.0, tokenizers==0.13.2", "example_code": "", "performance": {"dataset": "diode-subset", "accuracy": {"Loss": 0.436, "Mae": 0.4251, "Rmse": 0.6169, "Abs Rel": 0.45, "Log Mae": 0.1721, "Log Rmse": 0.2269, "Delta1": 0.3828, "Delta2": 0.6326, "Delta3": 0.8051}}, "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset. It is used for depth estimation in computer vision tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/resnet-50", "api_call": "ResNetForImageClassification.from_pretrained('microsoft/resnet-50')", "api_arguments": {"from_pretrained": "microsoft/resnet-50"}, "python_environment_requirements": {"transformers": "AutoImageProcessor, ResNetForImageClassification", "torch": "torch", "datasets": "load_dataset"}, "example_code": "from transformers import AutoImageProcessor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nprocessor = AutoImageProcessor.from_pretrained(microsoft/resnet-50)\nmodel = ResNetForImageClassification.from_pretrained(microsoft/resnet-50)\ninputs = processor(image, return_tensors=pt)\nwith torch.no_grad():\n logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])", "performance": {"dataset": "imagenet-1k", "accuracy": "~0.5% top1"}, "description": "ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/convnext-large-224", "api_call": "ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')", "api_arguments": {"pretrained_model_name_or_path": "facebook/convnext-large-224"}, "python_environment_requirements": {"transformers": "Hugging Face Transformers", "torch": "PyTorch", "datasets": "Hugging Face Datasets"}, "example_code": {"import": ["from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification", "import torch", "from datasets import load_dataset"], "load_dataset": "dataset = load_dataset('huggingface/cats-image')", "image": "image = dataset['test']['image'][0]", "feature_extractor": "feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')", "model": "model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')", "inputs": "inputs = feature_extractor(image, return_tensors='pt')", "logits": "with torch.no_grad():\n  logits = model(**inputs).logits", "predicted_label": "predicted_label = logits.argmax(-1).item()", "print": "print(model.config.id2label[predicted_label])"}, "performance": {"dataset": "imagenet-1k", "accuracy": "Not specified"}, "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/resnet-18", "api_call": "ResNetForImageClassification.from_pretrained('microsoft/resnet-18')", "api_arguments": ["image", "return_tensors"], "python_environment_requirements": ["transformers", "torch", "datasets"], "example_code": "from transformers import AutoFeatureExtractor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset('huggingface/cats-image')\nimage = dataset['test']['image'][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\nmodel = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\ninputs = feature_extractor(image, return_tensors='pt')\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])", "performance": {"dataset": "imagenet-1k"}, "description": "ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/convnext-base-224", "api_call": "ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')", "api_arguments": ["image", "return_tensors"], "python_environment_requirements": ["transformers", "torch", "datasets"], "example_code": "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset('huggingface/cats-image')\nimage = dataset['test']['image'][0]\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\ninputs = feature_extractor(image, return_tensors='pt')\nwith torch.no_grad():\n logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])", "performance": {"dataset": "imagenet-1k", "accuracy": null}, "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/beit-base-patch16-224-pt22k-ft22k", "api_call": "BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')", "api_arguments": {"images": "image", "return_tensors": "pt"}, "python_environment_requirements": "transformers", "example_code": "from transformers import BeitImageProcessor, BeitForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])", "performance": {"dataset": "ImageNet-22k", "accuracy": "Not specified"}, "description": "BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/vit-base-patch16-224", "api_call": "ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')", "api_arguments": {"pretrained_model_name_or_path": "google/vit-base-patch16-224", "from_tf": "False", "config": "None", "cache_dir": "None", "revision": "None", "use_auth_token": "False"}, "python_environment_requirements": {"transformers": "4.0.0", "torch": "1.9.0", "PIL": "8.3.2", "requests": "2.26.0"}, "example_code": {"1": "from transformers import ViTImageProcessor, ViTForImageClassification", "2": "from PIL import Image", "3": "import requests", "4": "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'", "5": "image = Image.open(requests.get(url, stream=True).raw)", "6": "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')", "7": "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')", "8": "inputs = processor(images=image, return_tensors='pt')", "9": "outputs = model(**inputs)", "10": "logits = outputs.logits", "11": "predicted_class_idx = logits.argmax(-1).item()", "12": "print('Predicted class:', model.config.id2label[predicted_class_idx])"}, "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "martinezomg/vit-base-patch16-224-diabetic-retinopathy", "api_call": "pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')", "api_arguments": {"model_name": "martinezomg/vit-base-patch16-224-diabetic-retinopathy"}, "python_environment_requirements": {"transformers": "4.28.1", "pytorch": "2.0.0+cu118", "datasets": "2.11.0", "tokenizers": "0.13.3"}, "example_code": "from transformers import pipeline\nimage_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\nresult = image_classifier('path/to/image.jpg')", "performance": {"dataset": "None", "accuracy": 0.7744}, "description": "This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Age Classification", "api_name": "nateraw/vit-age-classifier", "api_call": "ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')", "api_arguments": {"pretrained_model_name_or_path": "nateraw/vit-age-classifier"}, "python_environment_requirements": ["requests", "PIL", "transformers"], "example_code": "import requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import ViTFeatureExtractor, ViTForImageClassification\n\nr = requests.get('https://github.com/dchen236/FairFace/blob/master/detected_faces/race_Asian_face0.jpg?raw=true')\nim = Image.open(BytesIO(r.content))\n\nmodel = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\ntransforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\n\ninputs = transforms(im, return_tensors='pt')\noutput = model(**inputs)\n\nproba = output.logits.softmax(1)\npreds = proba.argmax(1)", "performance": {"dataset": "fairface", "accuracy": null}, "description": "A vision transformer finetuned to classify the age of a given person's face."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/vit-base-patch16-384", "api_call": "ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')", "api_arguments": {"pretrained_model_name_or_path": "google/vit-base-patch16-384"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import ViTFeatureExtractor, ViTForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])", "performance": {"dataset": "ImageNet", "accuracy": "Refer to tables 2 and 5 of the original paper"}, "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/beit-base-patch16-224", "api_call": "BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')", "api_arguments": {"pretrained_model_name_or_path": "microsoft/beit-base-patch16-224"}, "python_environment_requirements": ["transformers"], "example_code": "from transformers import BeitImageProcessor, BeitForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\nmodel = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\ninputs = processor(images=image, return_tensors='pt')\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])", "performance": {"dataset": "ImageNet", "accuracy": "Refer to tables 1 and 2 of the original paper"}, "description": "BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "abhishek/autotrain-dog-vs-food", "api_call": "pipeline('image-classification', model='abhishek/autotrain-dog-vs-food')", "api_arguments": "image_path", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "sasha/dog-food", "accuracy": 0.998}, "description": "A pre-trained model for classifying images as either dog or food using Hugging Face's AutoTrain framework."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lysandre/tiny-vit-random", "api_call": "ViTForImageClassification.from_pretrained('lysandre/tiny-vit-random')", "api_arguments": "image_path", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny-vit-random model for image classification using Hugging Face Transformers."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "fxmarty/resnet-tiny-beans", "api_call": "pipeline('image-classification', model='fxmarty/resnet-tiny-beans')", "api_arguments": {"model": "fxmarty/resnet-tiny-beans"}, "python_environment_requirements": {"transformers": "latest"}, "example_code": "from transformers import pipeline; classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans'); results = classifier('path/to/image.jpg')", "performance": {"dataset": "beans", "accuracy": "Not provided"}, "description": "A model trained on the beans dataset, just for testing and having a really tiny model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/mobilenet_v1_0.75_192", "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v1_0.75_192')", "api_arguments": {"pretrained_model_name_or_path": "google/mobilenet_v1_0.75_192"}, "python_environment_requirements": ["transformers"], "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v1_0.75_192)\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v1_0.75_192)\ninputs = preprocessor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "nvidia/mit-b0", "api_call": "SegformerForImageClassification.from_pretrained('nvidia/mit-b0')", "api_arguments": {"pretrained_model_name_or_path": "nvidia/mit-b0"}, "python_environment_requirements": {"transformers": "latest", "PIL": "latest", "requests": "latest"}, "example_code": "from transformers import SegformerFeatureExtractor, SegformerForImageClassification\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\nmodel = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\ninputs = feature_extractor(images=image, return_tensors='pt')\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint('Predicted class:', model.config.id2label[predicted_class_idx])", "performance": {"dataset": "imagenet_1k", "accuracy": "Not provided"}, "description": "SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/convnext-tiny-224", "api_call": "ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')", "api_arguments": {"pretrained_model_name_or_path": "facebook/convnext-tiny-224"}, "python_environment_requirements": ["transformers", "torch", "datasets"], "example_code": "from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset('huggingface/cats-image')\nimage = dataset['test']['image'][0]\nfeature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\nmodel = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224')\ninputs = feature_extractor(image, return_tensors='pt')\nwith torch.no_grad():\n logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])", "performance": {"dataset": "imagenet-1k", "accuracy": "Not specified"}, "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "vit_base_patch16_224.augreg2_in21k_ft_in1k", "api_call": "ViTForImageClassification.from_pretrained('timm/vit_base_patch16_224.augreg2_in21k_ft_in1k')", "api_arguments": {"pretrained_model_name_or_path": "timm/vit_base_patch16_224.augreg2_in21k_ft_in1k"}, "python_environment_requirements": ["transformers", "torch"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/mobilenet_v2_1.0_224", "api_call": "AutoModelForImageClassification.from_pretrained('google/mobilenet_v2_1.0_224')", "api_arguments": {"images": "image", "return_tensors": "pt"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\npreprocessor = AutoImageProcessor.from_pretrained(google/mobilenet_v2_1.0_224)\nmodel = AutoModelForImageClassification.from_pretrained(google/mobilenet_v2_1.0_224)\ninputs = preprocessor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])", "performance": {"dataset": "imagenet-1k", "accuracy": "Not specified"}, "description": "MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k", "api_call": "pipeline('image-classification', model='timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k', framework='pt')", "api_arguments": {"model": "timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k", "framework": "pt"}, "python_environment_requirements": ["transformers", "torch"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A ViT-based image classification model trained on ImageNet-1K and fine-tuned on ImageNet-12K by OpenAI."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "vit_tiny_patch16_224.augreg_in21k_ft_in1k", "api_call": "timm.create_model('hf_hub:timm/vit_tiny_patch16_224.augreg_in21k_ft_in1k')", "api_arguments": "pretrained", "python_environment_requirements": "timm", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentations and regularization."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/swin-tiny-patch4-window7-224", "api_call": "SwinForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224')", "api_arguments": {"images": "image", "return_tensors": "pt"}, "python_environment_requirements": {"transformers": "AutoFeatureExtractor", "PIL": "Image", "requests": "requests"}, "example_code": "from transformers import AutoFeatureExtractor, SwinForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = AutoFeatureExtractor.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\nmodel = SwinForImageClassification.from_pretrained(microsoft/swin-tiny-patch4-window7-224)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])", "performance": {"dataset": "imagenet-1k", "accuracy": "Not specified"}, "description": "Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "julien-c/hotdog-not-hotdog", "api_call": "pipeline('image-classification', model='julien-c/hotdog-not-hotdog')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": 0.825}, "description": "A model that classifies images as hotdog or not hotdog."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/swinv2-tiny-patch4-window8-256", "api_call": "AutoModelForImageClassification.from_pretrained('microsoft/swinv2-tiny-patch4-window8-256')", "api_arguments": {"image": "http://images.cocodataset.org/val2017/000000039769.jpg"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\nmodel = AutoModelForImageClassification.from_pretrained(microsoft/swinv2-tiny-patch4-window8-256)\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\npredicted_class_idx = logits.argmax(-1).item()\nprint(Predicted class:, model.config.id2label[predicted_class_idx])", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "saltacc/anime-ai-detect", "api_call": "pipeline('image-classification', model='saltacc/anime-ai-detect')", "api_arguments": ["image"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "aibooru and imageboard sites", "accuracy": "96%"}, "description": "A BEiT classifier to see if anime art was made by an AI or a human."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "swin-tiny-patch4-window7-224-bottom_cleaned_data", "api_call": "AutoModelForImageClassification.from_pretrained('microsoft/swin-tiny-patch4-window7-224-bottom_cleaned_data')", "api_arguments": ["learning_rate", "train_batch_size", "eval_batch_size", "seed", "gradient_accumulation_steps", "total_train_batch_size", "optimizer", "lr_scheduler_type", "lr_scheduler_warmup_ratio", "num_epochs"], "python_environment_requirements": ["Transformers 4.28.1", "Pytorch 2.0.0+cu118", "Datasets 2.11.0", "Tokenizers 0.13.3"], "example_code": "", "performance": {"dataset": "imagefolder", "accuracy": 0.9726}, "description": "This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/table-transformer-structure-recognition", "api_call": "pipeline('object-detection', model='microsoft/table-transformer-structure-recognition')", "api_arguments": "", "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "PubTables1M", "accuracy": ""}, "description": "Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/regnet-y-008", "api_call": "RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')", "api_arguments": {"pretrained_model_name_or_path": "zuppif/regnet-y-040"}, "python_environment_requirements": {"transformers": "AutoFeatureExtractor, RegNetForImageClassification", "torch": "torch", "datasets": "load_dataset"}, "example_code": "from transformers import AutoFeatureExtractor, RegNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(huggingface/cats-image)\nimage = dataset[test][image][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(zuppif/regnet-y-040)\nmodel = RegNetForImageClassification.from_pretrained(zuppif/regnet-y-040)\ninputs = feature_extractor(image, return_tensors=pt)\nwith torch.no_grad():\n... logits = model(**inputs).logits\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "convnextv2_huge.fcmae_ft_in1k", "api_call": "timm.create_model('convnextv2_huge.fcmae_ft_in1k')", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": ["timm"], "example_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))\nmodel = timm.create_model('convnextv2_huge.fcmae_ft_in1k', pretrained=True)\nmodel = model.eval()\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)", "performance": {"dataset": "imagenet-1k", "accuracy": 86.256}, "description": "A ConvNeXt-V2 image classification model. Pretrained with a fully convolutional masked autoencoder framework (FCMAE) and fine-tuned on ImageNet-1k."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification, Feature Map Extraction, Image Embeddings", "api_name": "convnext_base.fb_in1k", "api_call": "timm.create_model('convnext_base.fb_in1k')", "api_arguments": {"pretrained": "True", "features_only": "True", "num_classes": "0"}, "python_environment_requirements": ["timm"], "example_code": ["from urllib.request import urlopen", "from PIL import Image", "import timm", "img = Image.open(urlopen('https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'))", "model = timm.create_model('convnext_base.fb_in1k', pretrained=True)", "model = model.eval()", "data_config = timm.data.resolve_model_data_config(model)", "transforms = timm.data.create_transform(**data_config, is_training=False)", "output = model(transforms(img).unsqueeze(0))"], "performance": {"dataset": "imagenet-1k", "accuracy": "83.82%"}, "description": "A ConvNeXt image classification model pretrained on ImageNet-1k by paper authors. It can be used for image classification, feature map extraction, and image embeddings."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "timm/mobilenetv3_large_100.ra_in1k", "api_call": "timm.create_model('mobilenetv3_large_100.ra_in1k')", "api_arguments": {"pretrained": "True"}, "python_environment_requirements": {"timm": "latest"}, "example_code": "from urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('mobilenetv3_large_100.ra_in1k', pretrained=True)\nmodel = model.eval()\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "A MobileNet-v3 image classification model. Trained on ImageNet-1k in timm using recipe template described below. Recipe details: RandAugment RA recipe. Inspired by and evolved from EfficientNet RandAugment recipes. Published as B recipe in ResNet Strikes Back. RMSProp (TF 1.0 behaviour) optimizer, EMA weight averaging. Step (exponential decay w/ staircase) LR schedule with warmup."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/table-transformer-detection", "api_call": "TableTransformerDetrModel.from_pretrained('microsoft/table-transformer-detection')", "api_arguments": "image", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; table_detector = pipeline('object-detection', model='microsoft/table-transformer-detection'); results = table_detector(image)", "performance": {"dataset": "PubTables1M", "accuracy": "Not provided"}, "description": "Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "facebook/detr-resnet-50", "api_call": "DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')", "api_arguments": {"pretrained_model_name": "facebook/detr-resnet-50"}, "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "example_code": "from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-50)\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-50)\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)", "performance": {"dataset": "COCO 2017 validation", "accuracy": "42.0 AP"}, "description": "DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "hustvl/yolos-tiny", "api_call": "YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')", "api_arguments": {"images": "image", "return_tensors": "pt"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import YolosFeatureExtractor, YolosForObjectDetection\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\nmodel = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\nbboxes = outputs.pred_boxes", "performance": {"dataset": "COCO 2017 validation", "accuracy": "28.7 AP"}, "description": "YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "facebook/detr-resnet-101", "api_call": "DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101')", "api_arguments": ["image"], "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "example_code": "from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = DetrImageProcessor.from_pretrained(facebook/detr-resnet-101)\nmodel = DetrForObjectDetection.from_pretrained(facebook/detr-resnet-101)\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)", "performance": {"dataset": "COCO 2017", "accuracy": "43.5 AP"}, "description": "DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "zero-shot-object-detection", "api_name": "google/owlvit-base-patch32", "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch32')", "api_arguments": {"texts": "List of text queries", "images": "Image to be processed"}, "python_environment_requirements": "transformers", "example_code": "import requests\nfrom PIL import Image\nimport torch\nfrom transformers import OwlViTProcessor, OwlViTForObjectDetection\nprocessor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch32)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch32)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)", "performance": {"dataset": "COCO and OpenImages", "accuracy": "Not specified"}, "description": "OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Table Extraction", "api_name": "keremberke/yolov8m-table-extraction", "api_call": "YOLO('keremberke/yolov8m-table-extraction')", "api_arguments": {"image": "URL or local path to the image"}, "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-table-extraction')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "table-extraction", "accuracy": 0.952}, "description": "A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Detect Bordered and Borderless tables in documents", "api_name": "TahaDouaji/detr-doc-table-detection", "api_call": "DetrForObjectDetection.from_pretrained('TahaDouaji/detr-doc-table-detection')", "api_arguments": ["images", "return_tensors", "threshold"], "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "example_code": "from transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nimage = Image.open(IMAGE_PATH)\nprocessor = DetrImageProcessor.from_pretrained(TahaDouaji/detr-doc-table-detection)\nmodel = DetrForObjectDetection.from_pretrained(TahaDouaji/detr-doc-table-detection)\ninputs = processor(images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\nfor score, label, box in zip(results[scores], results[labels], results[boxes]):\n box = [round(i, 2) for i in box.tolist()]\n print(\n fDetected {model.config.id2label[label.item()]} with confidence \n f{round(score.item(), 3)} at location {box}\n )", "performance": {"dataset": "ICDAR2019 Table Dataset", "accuracy": "Not provided"}, "description": "detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "hustvl/yolos-small", "api_call": "YolosForObjectDetection.from_pretrained('hustvl/yolos-small')", "api_arguments": {"model_name": "hustvl/yolos-small"}, "python_environment_requirements": {"packages": ["transformers", "PIL", "requests"]}, "example_code": {"import": ["from transformers import YolosFeatureExtractor, YolosForObjectDetection", "from PIL import Image", "import requests"], "url": "http://images.cocodataset.org/val2017/000000039769.jpg", "image": "Image.open(requests.get(url, stream=True).raw)", "feature_extractor": "YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')", "model": "YolosForObjectDetection.from_pretrained('hustvl/yolos-small')", "inputs": "feature_extractor(images=image, return_tensors='pt')", "outputs": "model(**inputs)", "logits": "outputs.logits", "bboxes": "outputs.pred_boxes"}, "performance": {"dataset": "COCO 2017 validation", "accuracy": "36.1 AP"}, "description": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN)."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "facebook/detr-resnet-101-dc5", "api_call": "DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')", "api_arguments": {"image": "Image.open(requests.get(url, stream=True).raw)", "return_tensors": "pt"}, "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import DetrFeatureExtractor, DetrForObjectDetection\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\nmodel = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits\nbboxes = outputs.pred_boxes", "performance": {"dataset": "COCO 2017 validation", "accuracy": "AP 44.9"}, "description": "DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "deformable-detr", "api_call": "DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')", "api_arguments": ["images", "return_tensors"], "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "example_code": "from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\nmodel = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\ninputs = processor(images=image, return_tensors='pt')\noutputs = model(**inputs)", "performance": {"dataset": "COCO 2017", "accuracy": "Not provided"}, "description": "Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-hard-hat-detection", "api_call": "YOLO('keremberke/yolov8m-hard-hat-detection')", "api_arguments": {"image": "URL or local path to the image"}, "python_environment_requirements": ["ultralyticsplus==0.0.24", "ultralytics==8.0.23"], "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8m-hard-hat-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "hard-hat-detection", "accuracy": 0.811}, "description": "A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "License Plate Detection", "api_name": "keremberke/yolov5m-license-plate", "api_call": "yolov5.load('keremberke/yolov5m-license-plate')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic": false, "multi_label": false, "max_det": 1000, "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg", "size": 640, "augment": true}, "python_environment_requirements": "pip install -U yolov5", "example_code": ["import yolov5", "model = yolov5.load('keremberke/yolov5m-license-plate')", "model.conf = 0.25", "model.iou = 0.45", "model.agnostic = False", "model.multi_label = False", "model.max_det = 1000", "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model(img, size=640)", "results = model(img, augment=True)", "predictions = results.pred[0]", "boxes = predictions[:, :4]", "scores = predictions[:, 4]", "categories = predictions[:, 5]", "results.show()", "results.save(save_dir='results/')"], "performance": {"dataset": "keremberke/license-plate-object-detection", "accuracy": 0.988}, "description": "A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-valorant-detection", "api_call": "YOLO('keremberke/yolov8m-valorant-detection')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000}, "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21", "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-valorant-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "valorant-object-detection", "accuracy": 0.965}, "description": "A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-csgo-player-detection", "api_call": "YOLO('keremberke/yolov8m-csgo-player-detection')", "api_arguments": {"image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"}, "python_environment_requirements": "ultralyticsplus==0.0.23 ultralytics==8.0.21", "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8m-csgo-player-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "csgo-object-detection", "accuracy": 0.892}, "description": "An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Table Extraction", "api_name": "keremberke/yolov8s-table-extraction", "api_call": "YOLO('keremberke/yolov8s-table-extraction')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000, "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"}, "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21", "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8s-table-extraction')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "table-extraction", "accuracy": 0.984}, "description": "A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "zero-shot-object-detection", "api_name": "google/owlvit-large-patch14", "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-large-patch14')", "api_arguments": {"model_name": "google/owlvit-large-patch14"}, "python_environment_requirements": ["torch", "transformers", "PIL", "requests"], "example_code": ["import requests", "from PIL import Image", "import torch", "from transformers import OwlViTProcessor, OwlViTForObjectDetection", "processor = OwlViTProcessor.from_pretrained(google/owlvit-large-patch14)", "model = OwlViTForObjectDetection.from_pretrained(google/owlvit-large-patch14)", "url = http://images.cocodataset.org/val2017/000000039769.jpg", "image = Image.open(requests.get(url, stream=True).raw)", "texts = [[a photo of a cat, a photo of a dog]", "inputs = processor(text=texts, images=image, return_tensors=pt)", "outputs = model(**inputs)", "target_sizes = torch.Tensor([image.size[::-1]])", "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)", "i = 0", "text = texts[i]", "boxes, scores, labels = results[i][boxes], results[i][scores], results[i][labels]", "score_threshold = 0.1", "for box, score, label in zip(boxes, scores, labels):", " box = [round(i, 2) for i in box.tolist()]", " if score >= score_threshold:", " print(fDetected {text[label]} with confidence {round(score.item(), 3)} at location {box})"], "performance": {"dataset": "COCO", "accuracy": "Not specified"}, "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-nlf-head-detection", "api_call": "YOLO('keremberke/yolov8m-nlf-head-detection')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000, "image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"}, "python_environment_requirements": "pip install ultralyticsplus==0.0.24 ultralytics==8.0.23", "example_code": ["from ultralyticsplus import YOLO, render_result", "model = YOLO('keremberke/yolov8m-nlf-head-detection')", "model.overrides['conf'] = 0.25", "model.overrides['iou'] = 0.45", "model.overrides['agnostic_nms'] = False", "model.overrides['max_det'] = 1000", "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model.predict(image)", "print(results[0].boxes)", "render = render_result(model=model, image=image, result=results[0])", "render.show()"], "performance": {"dataset": "nfl-object-detection", "accuracy": 0.287}, "description": "A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-forklift-detection", "api_call": "YOLO('keremberke/yolov8m-forklift-detection')", "api_arguments": {"image": "URL or local path to the image"}, "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "example_code": ["from ultralyticsplus import YOLO, render_result", "model = YOLO('keremberke/yolov8m-forklift-detection')", "model.overrides['conf'] = 0.25", "model.overrides['iou'] = 0.45", "model.overrides['agnostic_nms'] = False", "model.overrides['max_det'] = 1000", "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model.predict(image)", "print(results[0].boxes)", "render = render_result(model=model, image=image, result=results[0])", "render.show()"], "performance": {"dataset": "forklift-object-detection", "accuracy": 0.846}, "description": "A YOLOv8 model for detecting forklifts and persons in images."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "zero-shot-object-detection", "api_name": "google/owlvit-base-patch16", "api_call": "OwlViTForObjectDetection.from_pretrained('google/owlvit-base-patch16')", "api_arguments": ["texts", "images"], "python_environment_requirements": ["requests", "PIL", "torch", "transformers"], "example_code": "processor = OwlViTProcessor.from_pretrained(google/owlvit-base-patch16)\nmodel = OwlViTForObjectDetection.from_pretrained(google/owlvit-base-patch16)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[a photo of a cat, a photo of a dog]]\ninputs = processor(text=texts, images=image, return_tensors=pt)\noutputs = model(**inputs)\ntarget_sizes = torch.Tensor([image.size[::-1]])\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)", "performance": {"dataset": "COCO", "accuracy": "Not provided"}, "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-plane-detection", "api_call": "YOLO('keremberke/yolov8m-plane-detection')", "api_arguments": {"image": "URL or local path to the image"}, "python_environment_requirements": ["pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"], "example_code": ["from ultralyticsplus import YOLO, render_result", "model = YOLO('keremberke/yolov8m-plane-detection')", "model.overrides['conf'] = 0.25", "model.overrides['iou'] = 0.45", "model.overrides['agnostic_nms'] = False", "model.overrides['max_det'] = 1000", "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model.predict(image)", "print(results[0].boxes)", "render = render_result(model=model, image=image, result=results[0])", "render.show()"], "performance": {"dataset": "plane-detection", "accuracy": "0.995"}, "description": "A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8s-csgo-player-detection", "api_call": "YOLO('keremberke/yolov8s-csgo-player-detection')", "api_arguments": {"image": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg"}, "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8s-csgo-player-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "csgo-object-detection", "accuracy": 0.886}, "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead']."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-blood-cell-detection", "api_call": "YOLO('keremberke/yolov8m-blood-cell-detection')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000}, "python_environment_requirements": ["ultralyticsplus==0.0.24", "ultralytics==8.0.23"], "example_code": ["from ultralyticsplus import YOLO, render_result", "model = YOLO('keremberke/yolov8m-blood-cell-detection')", "model.overrides['conf'] = 0.25", "model.overrides['iou'] = 0.45", "model.overrides['agnostic_nms'] = False", "model.overrides['max_det'] = 1000", "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model.predict(image)", "print(results[0].boxes)", "render = render_result(model=model, image=image, result=results[0])", "render.show()"], "performance": {"dataset": "blood-cell-object-detection", "accuracy": 0.927}, "description": "A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8s-hard-hat-detection", "api_call": "YOLO('keremberke/yolov8s-hard-hat-detection')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000}, "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21", "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8s-hard-hat-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "hard-hat-detection", "accuracy": 0.834}, "description": "An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Transformers", "functionality": "Object Detection", "api_name": "fcakyon/yolov5s-v7.0", "api_call": "yolov5.load('fcakyon/yolov5s-v7.0')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic": false, "multi_label": false, "max_det": 1000, "img": "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg", "size": 640, "augment": true}, "python_environment_requirements": "pip install -U yolov5", "example_code": "import yolov5\nmodel = yolov5.load('fcakyon/yolov5s-v7.0')\nmodel.conf = 0.25\nmodel.iou = 0.45\nmodel.agnostic = False\nmodel.multi_label = False\nmodel.max_det = 1000\nimg = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model(img)\nresults = model(img, size=640)\nresults = model(img, augment=True)\npredictions = results.pred[0]\nboxes = predictions[:, :4]\nscores = predictions[:, 4]\ncategories = predictions[:, 5]\nresults.show()\nresults.save(save_dir='results/')", "performance": {"dataset": "detection-datasets/coco", "accuracy": null}, "description": "Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ù„Ø¯ÙŠÙ†Ø§ Ù†Ø§Ø¯ÙŠ Ø·Ù„Ø§Ø¨ÙŠ ÙŠØ¬ØªÙ…Ø¹ ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù„Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©. Ø²ÙˆØ¯Ù‡Ù… Ø¨Ø£Ø¯Ø§Ø© Ù„Ù„Ø¹Ø¨ Ø¶Ø¯ ÙˆÙƒÙŠÙ„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play ğŸ‘€", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Unity ML-Agents ÙˆØ­Ø²Ù…Ø© ml-agents ÙÙŠ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ.\n2.Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆÙƒÙŠÙ„ poca Ø§Ù„Ù…Ø¯Ø±Ø¨ 'Raiden-1001/poca-Soccerv7.1' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù…Ø± Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ 'downloads'.\n3. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ† Ø¨ØªÙ†Ø³ÙŠÙ‚ YAML (Ù…Ø«Ù„ your_configuration_file_path.yaml) ÙŠØµÙ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙÙŠ Ø¨ÙŠØ¦Ø© SoccerTwos. LunarLander-v2.\n4.Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± 'mlagents-learn' Ù…Ø¹ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆÙ…Ø¹Ø±Ù Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„Ø¨Ø¯Ø¡ Ø¨ÙŠØ¦Ø© Unity Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ù„Ø·Ù„Ø§Ø¨ Ù„Ø¹Ø¨ ÙƒØ±Ø© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¶Ø¯ Ø®ØµÙ… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ.\n5. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù„Ø§Ù…Ø© '--resume' Ù„Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù‚Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© API.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Table Extraction", "api_name": "keremberke/yolov8n-table-extraction", "api_call": "YOLO('keremberke/yolov8n-table-extraction')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000}, "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "example_code": ["from ultralyticsplus import YOLO, render_result", "model = YOLO('keremberke/yolov8n-table-extraction')", "model.overrides['conf'] = 0.25", "model.overrides['iou'] = 0.45", "model.overrides['agnostic_nms'] = False", "model.overrides['max_det'] = 1000", "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model.predict(image)", "print(results[0].boxes)", "render = render_result(model=model, image=image, result=results[0])", "render.show()"], "performance": {"dataset": "table-extraction", "accuracy": 0.967}, "description": "An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "clipseg-rd64-refined", "api_call": "pipeline('image-segmentation', model='CIDAS/clipseg-rd64-refined')", "api_arguments": {"model": "CIDAS/clipseg-rd64-refined"}, "python_environment_requirements": "transformers", "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by LÃ¼ddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8n-csgo-player-detection", "api_call": "YOLO('keremberke/yolov8n-csgo-player-detection')", "api_arguments": {"image": "URL or local path to image"}, "python_environment_requirements": "pip install ultralyticsplus==0.0.23 ultralytics==8.0.21", "example_code": "from ultralyticsplus import YOLO, render_result\n\nmodel = YOLO('keremberke/yolov8n-csgo-player-detection')\n\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\n\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n\nresults = model.predict(image)\n\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "csgo-object-detection", "accuracy": 0.844}, "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead']."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "License Plate Detection", "api_name": "keremberke/yolov5s-license-plate", "api_call": "yolov5.load('keremberke/yolov5s-license-plate')", "api_arguments": {"img": "image url or path", "size": "image resize dimensions", "augment": "optional, test time augmentation"}, "python_environment_requirements": "pip install -U yolov5", "example_code": ["import yolov5", "model = yolov5.load('keremberke/yolov5s-license-plate')", "model.conf = 0.25", "model.iou = 0.45", "model.agnostic = False", "model.multi_label = False", "model.max_det = 1000", "img = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'", "results = model(img, size=640)", "results = model(img, augment=True)", "predictions = results.pred[0]", "boxes = predictions[:, :4]", "scores = predictions[:, 4]", "categories = predictions[:, 5]", "results.show()", "results.save(save_dir='results/')"], "performance": {"dataset": "keremberke/license-plate-object-detection", "accuracy": 0.985}, "description": "A YOLOv5 based license plate detection model trained on a custom dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨ØªØµÙ…ÙŠÙ… Ù…Ø±ÙƒØ¨Ø§Øª Ø°Ø§ØªÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©ØŒ ÙˆÙ†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªÙ„ØªÙ‚Ø·Ù‡ ÙƒØ§Ù…ÙŠØ±Ø§ØªÙ†Ø§.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø²Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø«Ù„ 'AutoModelForImageClassification' Ù…Ù† Transformers Ùˆ'Image' Ù…Ù† PIL Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØ±Ø©.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ 'AutoModelForImageClassification.from_pretrained'.\n3. ØªÙ… Ø¶Ø¨Ø· Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ÙØ±Ø¹ÙŠØ© Ù„Ù„ØµÙ…Ø§Ù… Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ Ù„Ù…Ù‡Ø§Ù… ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¹Ù…Ù‚ ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ Ù…Ø«Ù„ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ Ù…Ø´Ù‡Ø¯ Ù…Ø§ Ù„Ù„Ù…Ø±ÙƒØ¨Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ù„Ø©.\n4. ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ø¯ÙŠØ± Ø¹Ù…Ù‚ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙÙŠ ØµÙˆØ±Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ø³ÙˆØ§Ø¡ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ø±Ø© Ø£Ùˆ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù.\n"}]}, {"instruction": "Ø¹Ù…ÙŠÙ„Ù†Ø§ ÙŠØ·ÙˆÙ‘Ø± ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù„Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¶Ù…ÙŠÙ† ÙˆØ¸ÙŠÙØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆØ±ÙŠØ¯ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© Ù…Ù† Ø­Ø²Ù…Ø© transformers.\n2. Ø«Ù…ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ pipeline Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ø§Ù‹ 'espnet/kan-bayashi_ljspeech_vits'. ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ljspeech ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ù…Ø±ÙƒØ¨.\n3. Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø£ÙŠ Ù†Øµ Ø¥Ù„Ù‰ Ø¯Ø§Ù„Ø© pipeline Ù‡Ø°Ù‡ Ù„ØªØ®Ù„ÙŠÙ‚ ØµÙˆØª Ø§Ù„ÙƒÙ„Ø§Ù…. Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø¯Ù…Ø¬Ù‡ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø°ÙˆÙŠ Ø§Ù„Ø¥Ø¹Ø§Ù‚Ø© Ø§Ù„Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "openmmlab/upernet-convnext-small", "api_call": "UperNetModel.from_pretrained('openmmlab/upernet-convnext-small')", "api_arguments": "N/A", "python_environment_requirements": "transformers", "example_code": "N/A", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}], {"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Blood Cell Detection", "api_name": "keremberke/yolov8n-blood-cell-detection", "api_call": "YOLO('keremberke/yolov8n-blood-cell-detection')", "api_arguments": {"conf": 0.25, "iou": 0.45, "agnostic_nms": false, "max_det": 1000}, "python_environment_requirements": "ultralyticsplus==0.0.23 ultralytics==8.0.21", "example_code": "from ultralyticsplus import YOLO, render_result\nmodel = YOLO('keremberke/yolov8n-blood-cell-detection')\nmodel.overrides['conf'] = 0.25\nmodel.overrides['iou'] = 0.45\nmodel.overrides['agnostic_nms'] = False\nmodel.overrides['max_det'] = 1000\nimage = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\nresults = model.predict(image)\nprint(results[0].boxes)\nrender = render_result(model=model, image=image, result=results[0])\nrender.show()", "performance": {"dataset": "blood-cell-object-detection", "accuracy": 0.893}, "description": "This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
[[{"instruction": "Ø£Ù†Ø´Ø¦ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ ÙˆÙ‡ÙŠ AutoModel Ùˆtokenizer Ù…Ù† Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª.\n2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ 'lewtun/tiny-random-mt5' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ from_pretrained Ù„ÙØ¦Ø© AutoModel.\n3. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© ØªØ¨Ø¯Ø£ Ø¨Ù€ \"ÙƒØ§Ù† ÙŠØ§Ù…Ø§ ÙƒØ§Ù† ÙÙŠ Ø£Ø±Ø¶ Ø¨Ø¹ÙŠØ¯Ø©\" Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡.\n"}]}, {"instruction": "Ø£Ù†Ø§ Ù…Ù‡ØªÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ÙˆØµÙ Ù†ØµÙŠ Ù„Ù…Ø´Ù‡Ø¯ ÙŠØµÙˆØ± Ø§Ù„Ø­ÙŠÙˆØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø¨Ø±ÙŠØ©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ ÙÙŠ Ø°Ù„ÙƒØŸ", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. Ø§Ø³ØªÙˆØ±Ø¯ Ø¯Ø§Ù„Ø© pipline Ù…Ù† Ù…ÙƒØªØ¨Ø© transformers Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Hugging Face.\n2. Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© pipline Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù†Øµ Ø¥Ù„Ù‰ ÙÙŠØ¯ÙŠÙˆ.\n3. Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ 'chavinlo/TempoFunk' Ù„Ù„ØªØ­Ù…ÙŠÙ„ØŒ ÙˆØ§Ù„Ø°ÙŠ ØµÙ…Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¯Ø®Ù„.\n4. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙ Ø§Ù„Ù…Ø´Ù‡Ø¯ Ø§Ù„Ø°ÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø´Ø£.\n5.Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„Ù†Ø§ØªØ¬ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ GPT Ø³ÙŠÙƒÙˆÙ† ÙˆØµÙÙ‹Ø§ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ„ÙŠØ³ ÙÙŠØ¯ÙŠÙˆ ÙØ¹Ù„ÙŠØŒ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ GPT ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨ØµØ±ÙŠØ©.\n"}]}, {"instruction": "ØªÙ‚ÙˆÙ… Ø´Ø±ÙƒØªÙ†Ø§ Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ù†ØªØ¬ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦ÙØŒ ÙˆÙ†Ø±ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø¬Ø¯ÙˆÙ„ÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Scikit-Learn Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n2. ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ¸ÙŠÙØ© `load` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø­Ø²Ù…Ø© joblib ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ 'path_to_folder/sklearn_model.joblib').\n3. Ø¨Ù…Ø¬Ø±Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¥Ø¬Ø±Ø§Ø¡ ØªÙ†Ø¨Ø¤Ø§Øª Ø¨Ø´Ø£Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù…Ø«Ù„ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©ØŒ Ø£Ùˆ Ø§Ù„Ù…Ø³Ù…Ù‰ Ø§Ù„ÙˆØ¸ÙŠÙÙŠØŒ Ø£Ùˆ Ø§Ù„Ù…ÙˆÙ‚Ø¹ØŒ ÙˆØ§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„ØªÙ‚Ø¯ÙŠØ± Ø±Ø§ØªØ¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©.\n4. ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§ØªØ¨ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ù…Ù†ØªØ¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ù„ØªØ²ÙˆÙŠØ¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨ØªÙ‚Ø¯ÙŠØ± Ù„Ù„Ø¯Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„ Ù„Ù‚Ø§Ø¦Ù…Ø© ÙˆØ¸Ø§Ø¦Ù Ù…Ø¹ÙŠÙ†Ø©.\n"}]}], {"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Semantic Segmentation", "api_name": "nvidia/segformer-b0-finetuned-ade-512-512", "api_call": "SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')", "api_arguments": {"images": "Image", "return_tensors": "pt"}, "python_environment_requirements": {"transformers": "SegformerImageProcessor, SegformerForSemanticSegmentation", "PIL": "Image", "requests": "requests"}, "example_code": "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\nfrom PIL import Image\nimport requests\nprocessor = SegformerImageProcessor.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\nmodel = SegformerForSemanticSegmentation.from_pretrained(nvidia/segformer-b0-finetuned-ade-512-512)\nurl = http://images.cocodataset.org/val2017/000000039769.jpg\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlogits = outputs.logits", "performance": {"dataset": "ADE20k", "accuracy": "Not provided"}, "description": "SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository."}]
"using these 3 (instructon-instance) pairs for the given API with this format : {\"domain\": \"\", \"framework\": \"\", \"functionality\": \"\", \"api_name\": \"\", \"api_call\": \"\", \"api_arguments\": \"\", \"python_environment_requirements\": \"\", \"example_code\": \"\", \"performance\": {\"dataset\": \"\", \"accuracy\": \"\"}, \"description\":Â \"\"} Generate 10 (instructon-instance) pairs and make sure the instructions and exeplination in Arabic also make the results like this format : {\"instruction\":\"\",\"instances\":[{\"API\":{\"domain\":\"\",\"framework\":\"\",\"functionality\":\"\",\"api_name\":\"\",\"api_call\":\"\",\"api_arguments\":,\"python_environment_requirements\":,\"example_code\":\"\",\"performance\":{\"dataset\":\"\",\"accuracy\":\"\"},\"description\":\"\"},\"output\":\"<<<domain>>>:\n<<<api_call>>>: \n<<<api_provider>>>: \n<<<explanation>>>:\n\"}]} note: (don't use the API name when generateÂ instrutions)"
{"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}
{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}
{"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}

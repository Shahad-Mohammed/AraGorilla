[[{"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "YituTech/conv-bert-base", "api_call": "AutoModel.from_pretrained('YituTech/conv-bert-base')", "api_arguments": "N/A", "python_environment_requirements": "transformers", "example_code": "N/A", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library."}]
[[{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}], {"domain": "Natural Language Processing Text Generation", "framework": "Transformers", "functionality": "Text Generation", "api_name": "facebook/opt-1.3b", "api_call": "pipeline('text-generation', model='facebook/opt-1.3b')", "api_arguments": ["text"], "python_environment_requirements": ["transformers"], "example_code": "from transformers import pipeline\ngenerator = pipeline('text-generation', model='facebook/opt-1.3b')\ngenerator(Hello, I'm am conscious and)", "performance": {"dataset": "BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit, CCNewsV2", "accuracy": "Not provided"}, "description": "OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, trained to roughly match the performance and sizes of the GPT-3 class of models. It can be used for prompting for evaluation of downstream tasks as well as text generation."}]
[[{"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vits8", "api_call": "ViTModel.from_pretrained('facebook/dino-vits8')", "api_arguments": ["images", "return_tensors"], "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "imagenet-1k", "accuracy": null}, "description": "Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository."}]
[[{"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vitb16", "api_call": "ViTModel.from_pretrained('facebook/dino-vitb16')", "api_arguments": {"pretrained_model_name_or_path": "facebook/dino-vitb16"}, "python_environment_requirements": {"transformers": "latest", "PIL": "latest", "requests": "latest"}, "example_code": "from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."}]
[[{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "api_call": "AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')", "api_arguments": "input_ids, attention_mask", "python_environment_requirements": "transformers", "example_code": "inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]", "performance": {"dataset": "UMLS", "accuracy": "N/A"}, "description": "SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output."}]
[[{"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}], {"domain": "Audio Automatic Speech Recognition", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "microsoft/wavlm-large", "api_call": "Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')", "api_arguments": "speech input", "python_environment_requirements": "transformers", "example_code": "To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.", "performance": {"dataset": "SUPERB benchmark", "accuracy": "state-of-the-art performance"}, "description": "WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks."}]
[[{"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}], {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}]
[[{"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}], {"domain": "Reinforcement Learning", "framework": "Unity ML-Agents Library", "functionality": "Train and play SoccerTwos", "api_name": "poca-SoccerTwosv2", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-SoccerTwosv2' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["ml-agents"], "example_code": "mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}]
[[{"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}], {"domain": "Audio Audio-to-Audio", "framework": "Hugging Face Transformers", "functionality": "Asteroid", "api_name": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k", "api_call": "pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')", "api_arguments": {"model": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k"}, "python_environment_requirements": ["transformers", "asteroid"], "example_code": "", "performance": {"dataset": "Libri1Mix", "si_sdr": 14.829670037349064, "si_sdr_imp": 11.379888731489366, "sdr": 15.395712644737149, "sdr_imp": 11.893049845524112, "sir": "Infinity", "sir_imp": "NaN", "sar": 15.395712644737149, "sar_imp": 11.893049845524112, "stoi": 0.9301948391058859, "stoi_imp": 0.13427501556534832}, "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset."}]
[[{"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}], {"domain": "Tabular Tabular Classification", "framework": "Joblib", "functionality": "Carbon Emissions", "api_name": "jwan2021/autotrain-jwan-autotrain1-1768961489", "api_call": "joblib.load('model.joblib')", "api_arguments": ["data"], "python_environment_requirements": ["json", "joblib", "pandas"], "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)", "performance": {"dataset": "Validation Metrics", "accuracy": 0.983}, "description": "Binary Classification model for Carbon Emissions prediction"}]
[[{"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}], {"domain": "Natural Language Processing Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "YituTech/conv-bert-base", "api_call": "AutoModel.from_pretrained('YituTech/conv-bert-base')", "api_arguments": "N/A", "python_environment_requirements": "transformers", "example_code": "N/A", "performance": {"dataset": "N/A", "accuracy": "N/A"}, "description": "A pre-trained ConvBERT model for feature extraction provided by YituTech, based on the Hugging Face Transformers library."}]
[[{"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}], {"domain": "Natural Language Processing Text Generation", "framework": "Transformers", "functionality": "Text Generation", "api_name": "facebook/opt-1.3b", "api_call": "pipeline('text-generation', model='facebook/opt-1.3b')", "api_arguments": ["text"], "python_environment_requirements": ["transformers"], "example_code": "from transformers import pipeline\ngenerator = pipeline('text-generation', model='facebook/opt-1.3b')\ngenerator(Hello, I'm am conscious and)", "performance": {"dataset": "BookCorpus, CC-Stories, The Pile, Pushshift.io Reddit, CCNewsV2", "accuracy": "Not provided"}, "description": "OPT (Open Pre-trained Transformers) is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, trained to roughly match the performance and sizes of the GPT-3 class of models. It can be used for prompting for evaluation of downstream tasks as well as text generation."}]
[[{"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vits8", "api_call": "ViTModel.from_pretrained('facebook/dino-vits8')", "api_arguments": ["images", "return_tensors"], "python_environment_requirements": ["transformers", "PIL", "requests"], "example_code": "from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\nmodel = ViTModel.from_pretrained('facebook/dino-vits8')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "imagenet-1k", "accuracy": null}, "description": "Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository."}]
[[{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}], {"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vitb16", "api_call": "ViTModel.from_pretrained('facebook/dino-vitb16')", "api_arguments": {"pretrained_model_name_or_path": "facebook/dino-vitb16"}, "python_environment_requirements": {"transformers": "latest", "PIL": "latest", "requests": "latest"}, "example_code": "from transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vitb16')\nmodel = ViTModel.from_pretrained('facebook/dino-vitb16')\ninputs = feature_extractor(images=image, return_tensors=pt)\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state", "performance": {"dataset": "imagenet-1k", "accuracy": "Not provided"}, "description": "Vision Transformer (ViT) model trained using the DINO method. The model is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Note that this model does not include any fine-tuned heads."}]
[[{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}], {"domain": "Multimodal Feature Extraction", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "cambridgeltl/SapBERT-from-PubMedBERT-fulltext", "api_call": "AutoModel.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')", "api_arguments": "input_ids, attention_mask", "python_environment_requirements": "transformers", "example_code": "inputs = tokenizer('covid infection', return_tensors='pt'); outputs = model(**inputs); cls_embedding = outputs.last_hidden_state[:, 0, :]", "performance": {"dataset": "UMLS", "accuracy": "N/A"}, "description": "SapBERT is a pretraining scheme that self-aligns the representation space of biomedical entities. It is trained with UMLS 2020AA (English only) and uses microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext as the base model. The input should be a string of biomedical entity names, and the [CLS] embedding of the last layer is regarded as the output."}]
[[{"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}], {"domain": "Audio Automatic Speech Recognition", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "microsoft/wavlm-large", "api_call": "Wav2Vec2Model.from_pretrained('microsoft/wavlm-large')", "api_arguments": "speech input", "python_environment_requirements": "transformers", "example_code": "To fine-tune the model for speech recognition, see the official speech recognition example. To fine-tune the model for speech classification, see the official audio classification example.", "performance": {"dataset": "SUPERB benchmark", "accuracy": "state-of-the-art performance"}, "description": "WavLM-Large is a large model pretrained on 16kHz sampled speech audio. It is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. WavLM is pretrained on 60,000 hours of Libri-Light, 10,000 hours of GigaSpeech, and 24,000 hours of VoxPopuli. It achieves state-of-the-art performance on the SUPERB benchmark and brings significant improvements for various speech processing tasks on their representative benchmarks."}]
[[{"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}], {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}]
[[{"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}], {"domain": "Reinforcement Learning", "framework": "Unity ML-Agents Library", "functionality": "Train and play SoccerTwos", "api_name": "poca-SoccerTwosv2", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-SoccerTwosv2' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["ml-agents"], "example_code": "mlagents-learn <your_configuration_file_path.yaml> --run-id=<run_id> --resume", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "A trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}]
[[{"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}, {"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}], {"domain": "Audio Audio-to-Audio", "framework": "Hugging Face Transformers", "functionality": "Asteroid", "api_name": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k", "api_call": "pipeline('audio-to-audio', model='JorisCos/DPTNet_Libri1Mix_enhsingle_16k')", "api_arguments": {"model": "JorisCos/DPTNet_Libri1Mix_enhsingle_16k"}, "python_environment_requirements": ["transformers", "asteroid"], "example_code": "", "performance": {"dataset": "Libri1Mix", "si_sdr": 14.829670037349064, "si_sdr_imp": 11.379888731489366, "sdr": 15.395712644737149, "sdr_imp": 11.893049845524112, "sir": "Infinity", "sir_imp": "NaN", "sar": 15.395712644737149, "sar_imp": 11.893049845524112, "stoi": 0.9301948391058859, "stoi_imp": 0.13427501556534832}, "description": "This model was trained by Joris Cosentino using the librimix recipe in Asteroid. It was trained on the enh_single task of the Libri1Mix dataset."}]
[[{"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}], {"domain": "Tabular Tabular Classification", "framework": "Joblib", "functionality": "Carbon Emissions", "api_name": "jwan2021/autotrain-jwan-autotrain1-1768961489", "api_call": "joblib.load('model.joblib')", "api_arguments": ["data"], "python_environment_requirements": ["json", "joblib", "pandas"], "example_code": "import json\nimport joblib\nimport pandas as pd\nmodel = joblib.load('model.joblib')\nconfig = json.load(open('config.json'))\nfeatures = config['features']\ndata = pd.read_csv('data.csv')\ndata = data[features]\ndata.columns = ['feat_' + str(col) for col in data.columns]\npredictions = model.predict(data)", "performance": {"dataset": "Validation Metrics", "accuracy": 0.983}, "description": "Binary Classification model for Carbon Emissions prediction"}]
[[{"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}], {"domain": "Natural Language Processing Summarization", "framework": "Hugging Face Transformers", "functionality": "Text Summarization", "api_name": "facebook/bart-large-cnn", "api_call": "pipeline('summarization')", "api_arguments": ["ARTICLE", "max_length", "min_length", "do_sample"], "python_environment_requirements": ["transformers"], "example_code": "from transformers import pipeline\nsummarizer = pipeline(summarization, model=facebook/bart-large-cnn)\nARTICLE = ...\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))", "performance": {"dataset": "cnn_dailymail", "accuracy": {"ROUGE-1": 42.949, "ROUGE-2": 20.815, "ROUGE-L": 30.619, "ROUGE-LSUM": 40.038}}, "description": "BART (large-sized model), fine-tuned on CNN Daily Mail. BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs."}]
[[{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}, {"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}], {"domain": "Natural Language Processing Table Question Answering", "framework": "Transformers", "functionality": "Table Question Answering", "api_name": "google/tapas-large-finetuned-wtq", "api_call": "pipeline('table-question-answering')", "api_arguments": {"model": "google/tapas-large-finetuned-wtq", "task": "table-question-answering"}, "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline\n\nqa_pipeline = pipeline('table-question-answering', model='google/tapas-large-finetuned-wtq')\n\nresult = qa_pipeline(table=table, query=query)", "performance": {"dataset": "wikitablequestions", "accuracy": 0.5097}, "description": "TAPAS large model fine-tuned on WikiTable Questions (WTQ). This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table)."}]
[[{"instruction": "لدينا نادي طلابي يجتمع كل أسبوعين للعب كرة القدم الافتراضية. زودهم بأداة للعب ضد وكيل التعلم الآلي.", "instances": [{"API": {"domain": "Reinforcement Learning", "framework": "ML-Agents", "functionality": "SoccerTwos", "api_name": "Raiden-1001/poca-Soccerv7", "api_call": "mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'", "api_arguments": ["your_configuration_file_path.yaml", "run_id"], "python_environment_requirements": ["unity-ml-agents", "deep-reinforcement-learning", "ML-Agents-SoccerTwos"], "example_code": "Step 1: Write your model_id: Raiden-1001/poca-Soccerv7\nStep 2: Select your .nn /.onnx file\nClick on Watch the agent play 👀", "performance": {"dataset": "SoccerTwos", "accuracy": "Not provided"}, "description": "This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library."}, "output": "<<<domain>>>: Reinforcement Learning\n<<<api_call>>>: mlagents-load-from-hf --repo-id='Raiden-1001/poca-Soccerv7.1' --local-dir='./downloads'\n<<<api_provider>>>: Unity ML-Agents\n<<<explanation>>>: 1.تأكد من تثبيت Unity ML-Agents وحزمة ml-agents في بيئة Python الخاصة بك.\n2.قم بتنزيل نموذج وكيل poca المدرب 'Raiden-1001/poca-Soccerv7.1' باستخدام أمر استدعاء API المقدم. سيؤدي ذلك إلى حفظ الملفات في مجلد 'downloads'.\n3. ستحتاج إلى إنشاء ملف تكوين بتنسيق YAML (مثل your_configuration_file_path.yaml) يصف إعداد التدريب والمعلمات الفائقة المطلوبة لتشغيل الوكيل في بيئة SoccerTwos. LunarLander-v2.\n4.استخدم الأمر 'mlagents-learn' مع مسار ملف التكوين ومعرف التشغيل لبدء بيئة Unity حيث يمكن للطلاب لعب كرة القدم الافتراضية ضد خصم ذكاء اصطناعي.\n5. تأكد من استخدام العلامة '--resume' لبدء عملية التعلم من النموذج المقدم بواسطة API.\n"}]}, {"instruction": "أنشئ قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\".", "instances": [{"API": {"domain": "Natural Language Processing Text Generation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lewtun/tiny-random-mt5", "api_call": "AutoModel.from_pretrained('lewtun/tiny-random-mt5')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "nlp('Once upon a time...')", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random mt5 model for text generation"}, "output": "<<<domain>>>: Natural Language Processing Text Generation\n<<<api_call>>>: AutoModel.from_pretrained('lewtun/tiny-random-mt5')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>:1. قم باستيراد الفئات المناسبة، وهي AutoModel وtokenizer من مكتبة المحولات.\n2. قم بتحميل نموذج إنشاء النص الذي تم تدريبه مسبقًا 'lewtun/tiny-random-mt5' باستخدام أسلوب from_pretrained لفئة AutoModel.\n3. قم بإنشاء قصة قصيرة تبدأ بـ \"كان ياما كان في أرض بعيدة\" عن طريق تمرير موجه البداية إلى النموذج الذي تم تحميله.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}], {"domain": "Natural Language Processing Text2Text Generation", "framework": "Transformers", "functionality": "Text Generation", "api_name": "google/t5-v1_1-base", "api_call": "pipeline('text2text-generation')", "api_arguments": {"model": "google/t5-v1_1-base"}, "python_environment_requirements": {"transformers": ">=4.0.0"}, "example_code": "from transformers import pipeline\n\nt5 = pipeline('text2text-generation', model='google/t5-v1_1-base')\nt5('translate English to French: Hugging Face is a great company')", "performance": {"dataset": "c4", "accuracy": "Not provided"}, "description": "Google's T5 Version 1.1 is a state-of-the-art text-to-text transformer model that achieves high performance on various NLP tasks such as summarization, question answering, and text classification. It is pre-trained on the Colossal Clean Crawled Corpus (C4) and fine-tuned on downstream tasks."}]
[[{"instruction": "تقوم شركتنا ببناء منتج للبحث عن الوظائف، ونريد التنبؤ براتب الوظيفة بناءً على بعض مجموعات البيانات المتوفرة.", "instances": [{"API": {"domain": "Tabular Tabular Classification", "framework": "Scikit-learn", "functionality": "Joblib", "api_name": "julien-c/skops-digits", "api_call": "load('path_to_folder/sklearn_model.joblib')", "api_arguments": {"activation": "relu", "alpha": 0.0001, "batch_size": "auto", "beta_1": 0.9, "beta_2": 0.999, "early_stopping": "False", "epsilon": 1e-08, "hidden_layer_sizes": "(100,)", "learning_rate": "constant", "learning_rate_init": 0.001, "max_fun": 15000, "max_iter": 200, "momentum": 0.9, "n_iter_no_change": 10, "nesterovs_momentum": "True", "power_t": 0.5, "random_state": null, "shuffle": "True", "solver": "adam", "tol": 0.0001, "validation_fraction": 0.1, "verbose": "False", "warm_start": "False"}, "python_environment_requirements": "scikit-learn, joblib", "example_code": "This model can be loaded on the Inference API on-demand.", "performance": {"dataset": null, "accuracy": null}, "description": "A tabular classification model using the Scikit-learn framework and Joblib functionality. The model is trained with various hyperparameters and can be used for classification tasks."}, "output": "<<<domain>>>: Tabular Tabular Classification\n<<<api_call>>>: load('path_to_folder/sklearn_model.joblib')\n<<<api_provider>>>: Scikit-learn\n<<<explanation>>>:1. سنستخدم نموذج تصنيف جدولي تم تدريبه مسبقًا من Scikit-Learn للتنبؤ براتب الوظيفة بناءً على الميزات المتوفرة في مجموعة البيانات.\n2. يمكن تحميل النموذج باستخدام وظيفة `load` الخاصة بحزمة joblib وتحديد المسار حيث يتم تخزين النموذج (في هذه الحالة، 'path_to_folder/sklearn_model.joblib').\n3. بمجرد تحميل النموذج، يمكن استخدامه لإجراء تنبؤات بشأن بيانات الميزات، مثل سنوات الخبرة، أو المسمى الوظيفي، أو الموقع، والتي سيتم استخدامها بعد ذلك لتقدير راتب الوظيفة.\n4. يمكن استخدام الراتب المتوقع كجزء من منتج البحث عن وظيفة لتزويد المستخدمين بتقدير للدخل المحتمل لقائمة وظائف معينة.\n"}]}, {"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}], {"domain": "Natural Language Processing Token Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "kredor/punctuate-all", "api_call": "pipeline('token-classification')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "multilingual", "accuracy": 0.98}, "description": "A finetuned xlm-roberta-base model for punctuation prediction on twelve languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian."}]
[[{"instruction": "تقوم شركتنا بتصميم مركبات ذاتية القيادة، ونحتاج إلى تقدير عمق الأشياء في المشهد الذي تلتقطه كاميراتنا.", "instances": [{"API": {"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-DPTForDepthEstimation", "api_call": "DPTForDepthEstimation.from_pretrained('hf-tiny-model-private/tiny-random-DPTForDepthEstimation')", "api_arguments": [], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A tiny random DPT model for depth estimation using Hugging Face Transformers library."}, "output": "<<<domain>>>: Computer Vision Depth Estimation\n<<<api_call>>>: AutoModelForImageClassification.from_pretrained('sayakpaul/glpn-nyu-finetuned-diode-230131-041708')\n<<<api_provider>>>: Hugging Face Transformers\n<<<explanation>>>1. قم باستيراد الحزم المطلوبة، مثل 'AutoModelForImageClassification' من Transformers و'Image' من PIL لمعالجة بيانات الصورة.\n2. قم بتحميل النموذج الذي تم تدريبه مسبقًا 'sayakpaul/glpn-nyu-finetuned-diode-230131-041708' باستخدام الأسلوب 'AutoModelForImageClassification.from_pretrained'.\n3. تم ضبط هذا النموذج بدقة على مجموعة بيانات المجموعة الفرعية للصمام الثنائي لمهام تقدير العمق في سيناريوهات رؤية الكمبيوتر، مثل اكتشاف الكائنات في مشهد ما للمركبات المستقلة.\n4. يمكننا بعد ذلك استخدام النموذج لتقدير عمق الكائنات في صورة معينة، سواء تم التقاطها بواسطة الكاميرات الموجودة على السيارة أو تم تحميلها من ملف.\n"}]}, {"instruction": "أنا مهتم بتوليد فيديو من وصف نصي لمشهد يصور الحيوانات في البرية. هل يمكنك مساعدتي في ذلك؟", "instances": [{"API": {"domain": "Multimodal Text-to-Video", "framework": "Hugging Face", "functionality": "Text-to-Video", "api_name": "chavinlo/TempoFunk", "api_call": "pipeline('text-to-video', model='chavinlo/TempoFunk')", "api_arguments": ["input_text"], "python_environment_requirements": ["transformers"], "example_code": "", "performance": {"dataset": "", "accuracy": ""}, "description": "A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text."}, "output": "<<<domain>>>: Multimodal Text-to-Video\n<<<api_call>>>: pipeline('text-to-video', model='chavinlo/TempoFunk')\n<<<api_provider>>>: Hugging Face\n<<<explanation>>>: 1. استورد دالة pipline من مكتبة transformers المقدمة من Hugging Face.\n2. استخدم دالة pipline لإنشاء نموذج نص إلى فيديو.\n3. حدد النموذج 'chavinlo/TempoFunk' للتحميل، والذي صمم لتوليد محتوى الفيديو بناءً على النص المدخل.\n4. قم بتمرير وصف المشهد الذي ترغب في إنشاء فيديو له إلى النموذج المنشأ.\n5.لاحظ أن الناتج الذي يتم توليده من نموذج GPT سيكون وصفًا للفيديو وليس فيديو فعلي، لأن نماذج GPT تعتمد على إنتاج النصوص ولا يمكنها إنشاء مخرجات بصرية.\n"}]}, {"instruction": "عميلنا يطوّر تطبيقاً لذوي الإعاقة البصرية. نحن بحاجة إلى تضمين وظيفة تحويل النص إلى كلام لمستخدمي هذا التطبيق.", "instances": [{"API": {"domain": "Audio Text-to-Speech", "framework": "ESPnet", "functionality": "Text-to-Speech", "api_name": "kan-bayashi_ljspeech_vits", "api_call": "pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')", "api_arguments": "text", "python_environment_requirements": "transformers", "example_code": "from transformers import pipeline; tts = pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits'); tts('Hello World')", "performance": {"dataset": "ljspeech", "accuracy": "Not mentioned"}, "description": "A Text-to-Speech model trained on the ljspeech dataset using the ESPnet toolkit. This model can be used to convert text input into synthesized speech."}, "output": "<<<domain>>>: Audio Text-to-Speech\n<<<api_call>>>: pipeline('text-to-speech', model='espnet/kan-bayashi_ljspeech_vits')\n<<<api_provider>>>: ESPnet\n<<<explanation>>>:1. أولاً، نقوم بتوريد الفئة الضرورية من حزمة transformers.\n2. ثم، قم بإنشاء pipeline لتحويل النص إلى كلام باستخدام النموذج المدرب مسبقاً 'espnet/kan-bayashi_ljspeech_vits'. تم تدريب هذا النموذج على مجموعة بيانات ljspeech ويمكن استخدامه لتحويل النص إلى كلام مركب.\n3. الآن، قم بتمرير أي نص إلى دالة pipeline هذه لتخليق صوت الكلام. بعد ذلك، يمكنك تشغيل الصوت أو دمجه في تطبيقك لمستخدمي ذوي الإعاقة البصرية.\n"}]}], {"domain": "Multimodal Visual Question Answering", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/git-base-vqav2", "api_call": "pipeline('visual-question-answering')", "api_arguments": "image, question", "python_environment_requirements": ["transformers"], "example_code": "vqa(image='path/to/image.jpg', question='What is in the image?')", "performance": {"dataset": "VQAv2", "accuracy": "Refer to the paper for evaluation results"}, "description": "GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on VQAv2. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository."}]
